{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a3cf8b-8b2d-4565-964c-c9118c99ec23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch<3.0,>=2.2 in /usr/local/lib/python3.11/dist-packages (2.8.0.dev20250319+cu128)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.22.0.dev20250319+cu128)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0.dev20250319+cu128)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch<3.0,>=2.2) (77.0.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.2) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.2) (2.1.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.56.0-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting tqdm (from sentence-transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.8.0.dev20250319+cu128)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.9.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch>=1.11.0->sentence-transformers) (77.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m376.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m297.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m359.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading transformers-4.56.0-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m218.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading regex-2025.9.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.0/799.0 kB\u001b[0m \u001b[31m193.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m351.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m303.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m432.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, tqdm, threadpoolctl, safetensors, regex, numpy, joblib, hf-xet, scipy, pandas, huggingface-hub, tokenizers, scikit-learn, transformers, sentence-transformers\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.2\n",
      "    Uninstalling numpy-2.1.2:\n",
      "      Successfully uninstalled numpy-2.1.2\n",
      "Successfully installed hf-xet-1.1.9 huggingface-hub-0.34.4 joblib-1.5.2 numpy-2.3.2 pandas-2.3.2 pytz-2025.2 regex-2025.9.1 safetensors-0.6.2 scikit-learn-1.7.1 scipy-1.16.1 sentence-transformers-5.1.0 threadpoolctl-3.6.0 tokenizers-0.22.0 tqdm-4.67.1 transformers-4.56.0 tzdata-2025.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 설치 (Runpod A40 / 로컬)\n",
    "# ============================================\n",
    "\n",
    "# CPU 전용\n",
    "# python -m pip install -U \"torch>=2.2,<3.0\" scikit-learn pandas numpy joblib sentence-transformers transformers\n",
    "\n",
    "# GPU (CUDA 12.1, Runpod A40)\n",
    "!python -m pip install -U \"torch>=2.2,<3.0\" torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!python -m pip install -U scikit-learn pandas numpy joblib sentence-transformers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4839c95-1962-4a35-83db-ce8013559f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.56.0\n",
      "Uninstalling transformers-4.56.0:\n",
      "  Successfully uninstalled transformers-4.56.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: huggingface-hub 0.34.4\n",
      "Uninstalling huggingface-hub-0.34.4:\n",
      "  Successfully uninstalled huggingface-hub-0.34.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: tokenizers 0.22.0\n",
      "Uninstalling tokenizers-0.22.0:\n",
      "  Successfully uninstalled tokenizers-0.22.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformers\n",
      "  Downloading transformers-4.56.0-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.56.0-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m305.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m745.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m415.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.34.4 tokenizers-0.22.0 transformers-4.56.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.4-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.3.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.25.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.16.1)\n",
      "Downloading xgboost-3.0.4-py3-none-manylinux_2_28_x86_64.whl (94.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 MB\u001b[0m \u001b[31m371.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall transformers -y\n",
    "!pip uninstall huggingface-hub -y\n",
    "!pip uninstall tokenizers -y\n",
    "\n",
    "!pip install --no-cache-dir transformers\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb213f91-2d25-4e4e-bc06-829a6689d380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] cuda\n",
      "[Encoding Train Texts]\n",
      "[Encoding Validation Texts]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [06:59:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Best Thresholds per label]\n",
      "$$$: 0.50\n",
      "Amber: 0.22\n",
      "Aromatic: 0.20\n",
      "Blossom: 0.28\n",
      "Bouquet: 0.20\n",
      "Carnation: 0.20\n",
      "Citrus: 0.20\n",
      "Classical: 0.24\n",
      "Crisp: 0.20\n",
      "Dry: 0.20\n",
      "Floral: 0.30\n",
      "Flower: 0.20\n",
      "Fougère: 0.20\n",
      "Fresh: 0.20\n",
      "Fresher: 0.28\n",
      "Fruity: 0.20\n",
      "Gardenia: 0.20\n",
      "Gourmand: 0.20\n",
      "Green: 0.20\n",
      "Honeysuckle: 0.20\n",
      "Iris: 0.42\n",
      "Jasmine: 0.20\n",
      "Lilac: 0.20\n",
      "Lily: 0.26\n",
      "Magnolia: 0.20\n",
      "Mimosa: 0.20\n",
      "Mossy: 0.20\n",
      "Musk: 0.20\n",
      "Orange: 0.22\n",
      "Oriental: 0.20\n",
      "Rich: 0.40\n",
      "Richer: 0.22\n",
      "Rose: 0.20\n",
      "Soft: 0.20\n",
      "Spicy: 0.20\n",
      "Tuberose: 0.20\n",
      "Valley: 0.26\n",
      "Violet: 0.40\n",
      "Water: 0.20\n",
      "White: 0.20\n",
      "Woods: 0.20\n",
      "Woody: 0.20\n",
      "info: 0.20\n",
      "of: 0.50\n",
      "the: 0.50\n",
      "\n",
      "=== Threshold-based ===\n",
      "Micro-F1: 0.5001\n",
      "Macro-F1: 0.2513\n",
      "Sample-F1: 0.4985\n",
      "\n",
      "[classification_report @thr]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         $$$       0.00      0.00      0.00         1\n",
      "       Amber       0.41      0.84      0.55      1744\n",
      "    Aromatic       0.32      0.31      0.32       420\n",
      "     Blossom       0.60      0.12      0.19        26\n",
      "     Bouquet       0.00      0.00      0.00        39\n",
      "   Carnation       0.00      0.00      0.00         1\n",
      "      Citrus       0.32      0.45      0.37       993\n",
      "   Classical       0.35      0.61      0.44      1171\n",
      "       Crisp       0.31      0.34      0.32       896\n",
      "         Dry       0.48      0.16      0.23       251\n",
      "      Floral       0.63      0.82      0.72      2193\n",
      "      Flower       0.31      0.08      0.13       340\n",
      "     Fougère       0.32      0.31      0.32       420\n",
      "       Fresh       0.00      0.00      0.00        25\n",
      "     Fresher       0.58      0.96      0.72      2918\n",
      "      Fruity       0.36      0.49      0.42       961\n",
      "    Gardenia       1.00      0.40      0.57        10\n",
      "    Gourmand       0.36      0.19      0.25       340\n",
      "       Green       0.34      0.10      0.15       308\n",
      " Honeysuckle       0.00      0.00      0.00         0\n",
      "        Iris       1.00      0.03      0.06        31\n",
      "     Jasmine       0.85      0.50      0.63        22\n",
      "       Lilac       0.00      0.00      0.00         1\n",
      "        Lily       0.75      0.30      0.43        10\n",
      "    Magnolia       0.00      0.00      0.00         6\n",
      "      Mimosa       0.00      0.00      0.00         1\n",
      "       Mossy       0.50      0.03      0.05       255\n",
      "        Musk       0.50      0.17      0.26       109\n",
      "      Orange       0.50      0.12      0.19        26\n",
      "    Oriental       0.00      0.00      0.00         5\n",
      "        Rich       1.00      0.04      0.08        48\n",
      "      Richer       0.50      0.03      0.06       194\n",
      "        Rose       0.51      0.39      0.44       153\n",
      "        Soft       0.42      0.17      0.24       563\n",
      "       Spicy       0.00      0.00      0.00        43\n",
      "    Tuberose       0.82      0.43      0.56        21\n",
      "      Valley       0.67      0.29      0.40         7\n",
      "      Violet       1.00      0.12      0.22         8\n",
      "       Water       0.47      0.21      0.29       184\n",
      "       White       0.31      0.08      0.13       340\n",
      "       Woods       0.34      0.60      0.43      1099\n",
      "       Woody       0.32      0.33      0.33       691\n",
      "        info       0.00      0.00      0.00         1\n",
      "          of       0.67      0.29      0.40         7\n",
      "         the       0.67      0.29      0.40         7\n",
      "\n",
      "   micro avg       0.45      0.57      0.50     16889\n",
      "   macro avg       0.41      0.24      0.25     16889\n",
      "weighted avg       0.44      0.57      0.46     16889\n",
      " samples avg       0.46      0.58      0.50     16889\n",
      "\n",
      "\n",
      "[Example Prediction]\n",
      "['Amber', 'Fresher']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# MiniLM 임베딩 + VotingClassifier (앙상블)\n",
    "# + 하드코딩된 Threshold 적용(요청값 재현)\n",
    "# ============================================\n",
    "\n",
    "import os, numpy as np, pandas as pd, random\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC  # 참고: 사용 안함(soft 투표 불가)\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# -------------------------------\n",
    "# 설정\n",
    "# -------------------------------\n",
    "DATA_CSV = \"perfumes_huggingface.csv\"\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "TOP_K = 3\n",
    "RARE_MIN_COUNT = 7\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# 재현성 고정\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[Device] {device}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 유틸 함수\n",
    "# -------------------------------\n",
    "def split_labels(s: str):\n",
    "    s = str(s)\n",
    "    for sep in [\",\", \"|\", \"/\", \";\"]:\n",
    "        s = s.replace(sep, \" \")\n",
    "    return [t.strip() for t in s.split() if t.strip()]\n",
    "\n",
    "# -------------------------------\n",
    "# 1) 데이터 로드 & 전처리\n",
    "# -------------------------------\n",
    "df = pd.read_csv(DATA_CSV, sep=\"|\", engine=\"python\", on_bad_lines=\"skip\")\n",
    "df = df[~df[\"description\"].isna()].copy()\n",
    "df[\"labels\"] = df[\"fragrances\"].apply(split_labels)\n",
    "\n",
    "# 희소 라벨 제거\n",
    "cnt = Counter([l for L in df[\"labels\"] for l in L])\n",
    "rare = {k for k, v in cnt.items() if v <= RARE_MIN_COUNT}\n",
    "df[\"labels\"] = df[\"labels\"].apply(lambda L: [l for l in L if l not in rare])\n",
    "df = df[df[\"labels\"].map(len) > 0].copy()\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(df[\"labels\"])\n",
    "\n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
    "    df[\"description\"].tolist(), Y, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 2) MiniLM 임베딩 추출\n",
    "# -------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "base_model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "base_model.eval()\n",
    "\n",
    "def encode_texts(texts, batch_size=32):\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = base_model(**enc)\n",
    "            # mean-pooling\n",
    "            emb = out.last_hidden_state.mean(dim=1)\n",
    "        all_embeddings.append(emb.cpu().numpy())\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "print(\"[Encoding Train Texts]\")\n",
    "X_train_emb = encode_texts(X_train_text, batch_size=BATCH_SIZE)\n",
    "print(\"[Encoding Validation Texts]\")\n",
    "X_val_emb = encode_texts(X_val_text, batch_size=BATCH_SIZE)\n",
    "\n",
    "# -------------------------------\n",
    "# 3) VotingClassifier 앙상블 학습\n",
    "# -------------------------------\n",
    "logreg = LogisticRegression(max_iter=200)\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    use_label_encoder=False,\n",
    "    tree_method=\"hist\",  # (cuda일 때 gpu_hist로 바꿔도 되지만 결과 재현 위해 고정)\n",
    "    device=\"cuda\" if device == \"cuda\" else \"cpu\",\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "# LinearSVC는 predict_proba 없음 → soft voting 불가하므로 제외\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[(\"lr\", logreg), (\"xgb\", xgb)],\n",
    "    voting=\"soft\"\n",
    ")\n",
    "\n",
    "clf = OneVsRestClassifier(ensemble, n_jobs=-1)\n",
    "clf.fit(X_train_emb, y_train)\n",
    "\n",
    "# -------------------------------\n",
    "# 4) 검증 예측 & (하드코딩) Threshold 적용\n",
    "# -------------------------------\n",
    "y_val_proba = np.array(clf.predict_proba(X_val_emb))\n",
    "\n",
    "# === 너가 제공한 [Best Thresholds per label]을 고정 딕셔너리로 사용 ===\n",
    "fixed_thresholds = {\n",
    "    \"Amber\": 0.22,\n",
    "    \"Aromatic\": 0.20,\n",
    "    \"Blossom\": 0.28,\n",
    "    \"Bouquet\": 0.20,\n",
    "    \"Carnation\": 0.20,\n",
    "    \"Citrus\": 0.20,\n",
    "    \"Classical\": 0.24,\n",
    "    \"Crisp\": 0.20,\n",
    "    \"Dry\": 0.20,\n",
    "    \"Floral\": 0.30,\n",
    "    \"Flower\": 0.20,\n",
    "    \"Fougère\": 0.20,\n",
    "    \"Fresh\": 0.20,\n",
    "    \"Fresher\": 0.28,\n",
    "    \"Fruity\": 0.20,\n",
    "    \"Gardenia\": 0.20,\n",
    "    \"Gourmand\": 0.20,\n",
    "    \"Green\": 0.20,\n",
    "    \"Honeysuckle\": 0.20,\n",
    "    \"Iris\": 0.42,\n",
    "    \"Jasmine\": 0.20,\n",
    "    \"Lilac\": 0.20,\n",
    "    \"Lily\": 0.26,\n",
    "    \"Magnolia\": 0.20,\n",
    "    \"Mimosa\": 0.20,\n",
    "    \"Mossy\": 0.20,\n",
    "    \"Musk\": 0.20,\n",
    "    \"Orange\": 0.22,\n",
    "    \"Oriental\": 0.20,\n",
    "    \"Rich\": 0.40,\n",
    "    \"Richer\": 0.22,\n",
    "    \"Rose\": 0.20,\n",
    "    \"Soft\": 0.20,\n",
    "    \"Spicy\": 0.20,\n",
    "    \"Tuberose\": 0.20,\n",
    "    \"Valley\": 0.26,\n",
    "    \"Violet\": 0.40,\n",
    "    \"Water\": 0.20,\n",
    "    \"White\": 0.20,\n",
    "    \"Woods\": 0.20,\n",
    "    \"Woody\": 0.20,\n",
    "    \"info\": 0.20\n",
    "}\n",
    "\n",
    "# 라벨 순서(mlb.classes_)에 맞춰 적용\n",
    "y_val_pred_opt = np.zeros_like(y_val)\n",
    "print(\"\\n[Best Thresholds per label]\")\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    thr = fixed_thresholds.get(label, 0.5)\n",
    "    print(f\"{label}: {thr:.2f}\")\n",
    "    y_val_pred_opt[:, i] = (y_val_proba[:, i] >= thr).astype(int)\n",
    "\n",
    "# -------------------------------\n",
    "# 5) 평가 (요청값 재현)\n",
    "# -------------------------------\n",
    "micro_f1 = f1_score(y_val, y_val_pred_opt, average=\"micro\")\n",
    "macro_f1 = f1_score(y_val, y_val_pred_opt, average=\"macro\")\n",
    "sample_f1 = f1_score(y_val, y_val_pred_opt, average=\"samples\")\n",
    "\n",
    "print(\"\\n=== Threshold-based ===\")\n",
    "print(f\"Micro-F1: {micro_f1:.4f}\")\n",
    "print(f\"Macro-F1: {macro_f1:.4f}\")\n",
    "print(f\"Sample-F1: {sample_f1:.4f}\")\n",
    "\n",
    "print(\"\\n[classification_report @thr]\")\n",
    "print(classification_report(y_val, y_val_pred_opt, target_names=mlb.classes_, zero_division=0))\n",
    "\n",
    "# -------------------------------\n",
    "# 6) 예측 함수\n",
    "# -------------------------------\n",
    "def predict_multilingual(text: str, topk=3, thresholds=None):\n",
    "    emb = encode_texts([text], batch_size=1)\n",
    "    proba = clf.predict_proba(emb)[0]\n",
    "    if thresholds is not None:\n",
    "        pick = [i for i, p in enumerate(proba) if p >= thresholds.get(mlb.classes_[i], 0.5)]\n",
    "        if not pick:\n",
    "            pick = np.argsort(-proba)[:topk]\n",
    "    else:\n",
    "        pick = np.argsort(-proba)[:topk]\n",
    "    return [mlb.classes_[i] for i in pick]\n",
    "\n",
    "print(\"\\n[Example Prediction]\")\n",
    "print(predict_multilingual(\n",
    "    \"바닷가에서 느껴지는 시원하고 약간 달콤한 향이 좋아요\",\n",
    "    topk=TOP_K,\n",
    "    thresholds=fixed_thresholds\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82696de6-f66b-4f13-8200-9f14d97d7cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] cuda\n",
      "[Encoding Train Texts]\n",
      "[Encoding Validation Texts]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:41:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Best Thresholds per label]\n",
      "$$$: 0.50\n",
      "Amber: 0.22\n",
      "Aromatic: 0.20\n",
      "Blossom: 0.28\n",
      "Bouquet: 0.20\n",
      "Carnation: 0.20\n",
      "Citrus: 0.20\n",
      "Classical: 0.24\n",
      "Crisp: 0.20\n",
      "Dry: 0.20\n",
      "Floral: 0.30\n",
      "Flower: 0.20\n",
      "Fougère: 0.20\n",
      "Fresh: 0.20\n",
      "Fresher: 0.28\n",
      "Fruity: 0.20\n",
      "Gardenia: 0.20\n",
      "Gourmand: 0.20\n",
      "Green: 0.20\n",
      "Honeysuckle: 0.20\n",
      "Iris: 0.42\n",
      "Jasmine: 0.20\n",
      "Lilac: 0.20\n",
      "Lily: 0.26\n",
      "Magnolia: 0.20\n",
      "Mimosa: 0.20\n",
      "Mossy: 0.20\n",
      "Musk: 0.20\n",
      "Orange: 0.22\n",
      "Oriental: 0.20\n",
      "Rich: 0.40\n",
      "Richer: 0.22\n",
      "Rose: 0.20\n",
      "Soft: 0.20\n",
      "Spicy: 0.20\n",
      "Tuberose: 0.20\n",
      "Valley: 0.26\n",
      "Violet: 0.40\n",
      "Water: 0.20\n",
      "White: 0.20\n",
      "Woods: 0.20\n",
      "Woody: 0.20\n",
      "info: 0.50\n",
      "of: 0.50\n",
      "the: 0.50\n",
      "\n",
      "=== Threshold-based ===\n",
      "Micro-F1: 0.5006\n",
      "Macro-F1: 0.2501\n",
      "Sample-F1: 0.4996\n",
      "\n",
      "[classification_report @thr]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         $$$       0.00      0.00      0.00         1\n",
      "       Amber       0.41      0.85      0.55      1744\n",
      "    Aromatic       0.34      0.36      0.35       420\n",
      "     Blossom       0.50      0.08      0.13        26\n",
      "     Bouquet       0.00      0.00      0.00        39\n",
      "   Carnation       0.00      0.00      0.00         1\n",
      "      Citrus       0.31      0.47      0.37       993\n",
      "   Classical       0.35      0.63      0.45      1171\n",
      "       Crisp       0.30      0.35      0.32       896\n",
      "         Dry       0.42      0.16      0.23       251\n",
      "      Floral       0.63      0.83      0.71      2193\n",
      "      Flower       0.30      0.09      0.13       340\n",
      "     Fougère       0.34      0.36      0.35       420\n",
      "       Fresh       0.00      0.00      0.00        25\n",
      "     Fresher       0.58      0.96      0.72      2918\n",
      "      Fruity       0.36      0.53      0.43       961\n",
      "    Gardenia       1.00      0.40      0.57        10\n",
      "    Gourmand       0.34      0.20      0.25       340\n",
      "       Green       0.33      0.10      0.15       308\n",
      " Honeysuckle       0.00      0.00      0.00         0\n",
      "        Iris       0.50      0.03      0.06        31\n",
      "     Jasmine       0.73      0.50      0.59        22\n",
      "       Lilac       0.00      0.00      0.00         1\n",
      "        Lily       0.60      0.30      0.40        10\n",
      "    Magnolia       0.00      0.00      0.00         6\n",
      "      Mimosa       0.00      0.00      0.00         1\n",
      "       Mossy       0.50      0.03      0.05       255\n",
      "        Musk       0.51      0.21      0.30       109\n",
      "      Orange       0.38      0.12      0.18        26\n",
      "    Oriental       0.00      0.00      0.00         5\n",
      "        Rich       1.00      0.02      0.04        48\n",
      "      Richer       0.47      0.04      0.07       194\n",
      "        Rose       0.50      0.42      0.45       153\n",
      "        Soft       0.40      0.17      0.24       563\n",
      "       Spicy       0.00      0.00      0.00        43\n",
      "    Tuberose       0.77      0.48      0.59        21\n",
      "      Valley       0.50      0.29      0.36         7\n",
      "      Violet       0.50      0.12      0.20         8\n",
      "       Water       0.42      0.23      0.30       184\n",
      "       White       0.30      0.09      0.13       340\n",
      "       Woods       0.33      0.64      0.44      1099\n",
      "       Woody       0.30      0.34      0.32       691\n",
      "        info       0.00      0.00      0.00         1\n",
      "          of       0.67      0.29      0.40         7\n",
      "         the       0.67      0.29      0.40         7\n",
      "\n",
      "   micro avg       0.44      0.58      0.50     16889\n",
      "   macro avg       0.37      0.24      0.25     16889\n",
      "weighted avg       0.43      0.58      0.47     16889\n",
      " samples avg       0.45      0.60      0.50     16889\n",
      "\n",
      "\n",
      "[Example Prediction]\n",
      "['Amber', 'Fresher']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# MiniLM 임베딩 + VotingClassifier (앙상블)\n",
    "# + 하드코딩된 Threshold 적용(요청값 재현)\n",
    "# + 개선: LR에 class_weight=\"balanced\", 소프트보팅 가중치 조정\n",
    "# ============================================\n",
    "\n",
    "import os, numpy as np, pandas as pd, random\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC  # 참고: 사용 안함(soft 투표 불가)\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# -------------------------------\n",
    "# 설정\n",
    "# -------------------------------\n",
    "DATA_CSV = \"perfumes_huggingface.csv\"\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "TOP_K = 3\n",
    "RARE_MIN_COUNT = 7\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# 재현성 고정\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[Device] {device}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 유틸 함수\n",
    "# -------------------------------\n",
    "def split_labels(s: str):\n",
    "    s = str(s)\n",
    "    for sep in [\",\", \"|\", \"/\", \";\"]:\n",
    "        s = s.replace(sep, \" \")\n",
    "    return [t.strip() for t in s.split() if t.strip()]\n",
    "\n",
    "# -------------------------------\n",
    "# 1) 데이터 로드 & 전처리\n",
    "# -------------------------------\n",
    "df = pd.read_csv(DATA_CSV, sep=\"|\", engine=\"python\", on_bad_lines=\"skip\")\n",
    "df = df[~df[\"description\"].isna()].copy()\n",
    "df[\"labels\"] = df[\"fragrances\"].apply(split_labels)\n",
    "\n",
    "# 희소 라벨 제거\n",
    "cnt = Counter([l for L in df[\"labels\"] for l in L])\n",
    "rare = {k for k, v in cnt.items() if v <= RARE_MIN_COUNT}\n",
    "df[\"labels\"] = df[\"labels\"].apply(lambda L: [l for l in L if l not in rare])\n",
    "df = df[df[\"labels\"].map(len) > 0].copy()\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(df[\"labels\"])\n",
    "\n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
    "    df[\"description\"].tolist(), Y, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 2) MiniLM 임베딩 추출\n",
    "# -------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "base_model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "base_model.eval()\n",
    "\n",
    "def encode_texts(texts, batch_size=32):\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = base_model(**enc)\n",
    "            # mean-pooling\n",
    "            emb = out.last_hidden_state.mean(dim=1)\n",
    "        all_embeddings.append(emb.cpu().numpy())\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "print(\"[Encoding Train Texts]\")\n",
    "X_train_emb = encode_texts(X_train_text, batch_size=BATCH_SIZE)\n",
    "print(\"[Encoding Validation Texts]\")\n",
    "X_val_emb = encode_texts(X_val_text, batch_size=BATCH_SIZE)\n",
    "\n",
    "# -------------------------------\n",
    "# 3) VotingClassifier 앙상블 학습\n",
    "# -------------------------------\n",
    "logreg = LogisticRegression(max_iter=200)\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    use_label_encoder=False,\n",
    "    tree_method=\"hist\",  # (cuda일 때 gpu_hist로 바꿔도 가능하나 결과 재현 위해 고정)\n",
    "    device=\"cuda\" if device == \"cuda\" else \"cpu\",\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "# LinearSVC는 predict_proba 없음 → soft voting 불가하므로 제외\n",
    "# 소프트보팅에서 XGB 가중치 약간 ↑ (경험상 안정적 이득)\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[(\"lr\", logreg), (\"xgb\", xgb)],\n",
    "    voting=\"soft\",\n",
    "    weights=[1.3, 0.8]  # ← 가중치 조정(필요시 1.2~1.4 범위로 소폭 튜닝)\n",
    ")\n",
    "\n",
    "clf = OneVsRestClassifier(ensemble, n_jobs=-1)\n",
    "clf.fit(X_train_emb, y_train)\n",
    "\n",
    "# -------------------------------\n",
    "# 4) 검증 예측 & (하드코딩) Threshold 적용\n",
    "# -------------------------------\n",
    "y_val_proba = np.array(clf.predict_proba(X_val_emb))\n",
    "\n",
    "# === 제공된 [Best Thresholds per label]을 고정 딕셔너리로 사용 ===\n",
    "fixed_thresholds = {\n",
    "    \"Amber\": 0.22,\n",
    "    \"Aromatic\": 0.20,\n",
    "    \"Blossom\": 0.28,\n",
    "    \"Bouquet\": 0.20,\n",
    "    \"Carnation\": 0.20,\n",
    "    \"Citrus\": 0.20,\n",
    "    \"Classical\": 0.24,\n",
    "    \"Crisp\": 0.20,\n",
    "    \"Dry\": 0.20,\n",
    "    \"Floral\": 0.30,\n",
    "    \"Flower\": 0.20,\n",
    "    \"Fougère\": 0.20,\n",
    "    \"Fresh\": 0.20,\n",
    "    \"Fresher\": 0.28,\n",
    "    \"Fruity\": 0.20,\n",
    "    \"Gardenia\": 0.20,\n",
    "    \"Gourmand\": 0.20,\n",
    "    \"Green\": 0.20,\n",
    "    \"Honeysuckle\": 0.20,\n",
    "    \"Iris\": 0.42,\n",
    "    \"Jasmine\": 0.20,\n",
    "    \"Lilac\": 0.20,\n",
    "    \"Lily\": 0.26,\n",
    "    \"Magnolia\": 0.20,\n",
    "    \"Mimosa\": 0.20,\n",
    "    \"Mossy\": 0.20,\n",
    "    \"Musk\": 0.20,\n",
    "    \"Orange\": 0.22,\n",
    "    \"Oriental\": 0.20,\n",
    "    \"Rich\": 0.40,\n",
    "    \"Richer\": 0.22,\n",
    "    \"Rose\": 0.20,\n",
    "    \"Soft\": 0.20,\n",
    "    \"Spicy\": 0.20,\n",
    "    \"Tuberose\": 0.20,\n",
    "    \"Valley\": 0.26,\n",
    "    \"Violet\": 0.40,\n",
    "    \"Water\": 0.20,\n",
    "    \"White\": 0.20,\n",
    "    \"Woods\": 0.20,\n",
    "    \"Woody\": 0.20,\n",
    "}\n",
    "\n",
    "# 라벨 순서(mlb.classes_)에 맞춰 적용\n",
    "y_val_pred_opt = np.zeros_like(y_val)\n",
    "print(\"\\n[Best Thresholds per label]\")\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    thr = fixed_thresholds.get(label, 0.5)\n",
    "    print(f\"{label}: {thr:.2f}\")\n",
    "    y_val_pred_opt[:, i] = (y_val_proba[:, i] >= thr).astype(int)\n",
    "\n",
    "# -------------------------------\n",
    "# 5) 평가 (요청값 재현)\n",
    "# -------------------------------\n",
    "micro_f1 = f1_score(y_val, y_val_pred_opt, average=\"micro\")\n",
    "macro_f1 = f1_score(y_val, y_val_pred_opt, average=\"macro\")\n",
    "sample_f1 = f1_score(y_val, y_val_pred_opt, average=\"samples\")\n",
    "\n",
    "print(\"\\n=== Threshold-based ===\")\n",
    "print(f\"Micro-F1: {micro_f1:.4f}\")\n",
    "print(f\"Macro-F1: {macro_f1:.4f}\")\n",
    "print(f\"Sample-F1: {sample_f1:.4f}\")\n",
    "\n",
    "print(\"\\n[classification_report @thr]\")\n",
    "print(classification_report(y_val, y_val_pred_opt, target_names=mlb.classes_, zero_division=0))\n",
    "\n",
    "# -------------------------------\n",
    "# 6) 예측 함수\n",
    "# -------------------------------\n",
    "def predict_multilingual(text: str, topk=3, thresholds=None):\n",
    "    emb = encode_texts([text], batch_size=1)\n",
    "    proba = clf.predict_proba(emb)[0]\n",
    "    if thresholds is not None:\n",
    "        pick = [i for i, p in enumerate(proba) if p >= thresholds.get(mlb.classes_[i], 0.5)]\n",
    "        if not pick:\n",
    "            pick = np.argsort(-proba)[:topk]\n",
    "    else:\n",
    "        pick = np.argsort(-proba)[:topk]\n",
    "    return [mlb.classes_[i] for i in pick]\n",
    "\n",
    "print(\"\\n[Example Prediction]\")\n",
    "print(predict_multilingual(\n",
    "    \"바닷가에서 느껴지는 시원하고 약간 달콤한 향이 좋아요\",\n",
    "    topk=TOP_K,\n",
    "    thresholds=fixed_thresholds\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88939765-a227-438e-aab0-200a29d03491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] cuda\n",
      "[Encoding Train Texts]\n",
      "[Encoding Validation Texts]\n",
      "\n",
      "[Mini Search] Trying tiny grid of (weights, LR.C, XGB.reg_lambda)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:47:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  weights=(1.3, 0.8), C=1.0, reg_lambda=1.0  →  Micro 0.3090 | Sample 0.3128 | Macro 0.1956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [07:48:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  weights=(1.3, 0.8), C=1.0, reg_lambda=1.5  →  Micro 0.3090 | Sample 0.3128 | Macro 0.1955\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# MiniLM 임베딩 + VotingClassifier (앙상블)\n",
    "# + 하드코딩된 Threshold 적용(요청값 재현)\n",
    "# + 초소형 하이퍼파라미터 탐색(8조합): weights, LR.C, XGB.reg_lambda\n",
    "# ============================================\n",
    "\n",
    "import os, numpy as np, pandas as pd, random\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC  # 참고: 사용 안함(soft 투표 불가)\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# -------------------------------\n",
    "# 설정\n",
    "# -------------------------------\n",
    "DATA_CSV = \"perfumes_huggingface.csv\"\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "TOP_K = 3\n",
    "RARE_MIN_COUNT = 7\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# 재현성 고정\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[Device] {device}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 유틸 함수\n",
    "# -------------------------------\n",
    "def split_labels(s: str):\n",
    "    s = str(s)\n",
    "    for sep in [\",\", \"|\", \"/\", \";\"]:\n",
    "        s = s.replace(sep, \" \")\n",
    "    return [t.strip() for t in s.split() if t.strip()]\n",
    "\n",
    "# -------------------------------\n",
    "# 1) 데이터 로드 & 전처리\n",
    "# -------------------------------\n",
    "df = pd.read_csv(DATA_CSV, sep=\"|\", engine=\"python\", on_bad_lines=\"skip\")\n",
    "df = df[~df[\"description\"].isna()].copy()\n",
    "df[\"labels\"] = df[\"fragrances\"].apply(split_labels)\n",
    "\n",
    "# 희소 라벨 제거\n",
    "cnt = Counter([l for L in df[\"labels\"] for l in L])\n",
    "rare = {k for k, v in cnt.items() if v <= RARE_MIN_COUNT}\n",
    "df[\"labels\"] = df[\"labels\"].apply(lambda L: [l for l in L if l not in rare])\n",
    "df = df[df[\"labels\"].map(len) > 0].copy()\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(df[\"labels\"])\n",
    "\n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
    "    df[\"description\"].tolist(), Y, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 2) MiniLM 임베딩 추출 (1회만)\n",
    "# -------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "base_model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "base_model.eval()\n",
    "\n",
    "def encode_texts(texts, batch_size=32):\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = base_model(**enc)\n",
    "            emb = out.last_hidden_state.mean(dim=1)  # mean-pooling\n",
    "        all_embeddings.append(emb.cpu().numpy())\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "print(\"[Encoding Train Texts]\")\n",
    "X_train_emb = encode_texts(X_train_text, batch_size=BATCH_SIZE)\n",
    "print(\"[Encoding Validation Texts]\")\n",
    "X_val_emb = encode_texts(X_val_text, batch_size=BATCH_SIZE)\n",
    "\n",
    "# -------------------------------\n",
    "# 3) 초소형 하이퍼파라미터 탐색\n",
    "#    - weights: 2가지\n",
    "#    - LR.C: 2가지 (class_weight='balanced' 고정)\n",
    "#    - XGB.reg_lambda: 2가지 (경량 정규화만 탐색)\n",
    "#    총 8조합, Micro-F1로 선택 (고정 임계값 적용)\n",
    "# -------------------------------\n",
    "# 고정 임계값 (네가 준 값)\n",
    "fixed_thresholds = {\n",
    "    \"Amber\": 0.22, \"Aromatic\": 0.20, \"Blossom\": 0.28, \"Bouquet\": 0.20, \"Carnation\": 0.20,\n",
    "    \"Citrus\": 0.20, \"Classical\": 0.24, \"Crisp\": 0.20, \"Dry\": 0.20, \"Floral\": 0.30,\n",
    "    \"Flower\": 0.20, \"Fougère\": 0.20, \"Fresh\": 0.20, \"Fresher\": 0.28, \"Fruity\": 0.20,\n",
    "    \"Gardenia\": 0.20, \"Gourmand\": 0.20, \"Green\": 0.20, \"Honeysuckle\": 0.20, \"Iris\": 0.42,\n",
    "    \"Jasmine\": 0.20, \"Lilac\": 0.20, \"Lily\": 0.26, \"Magnolia\": 0.20, \"Mimosa\": 0.20,\n",
    "    \"Mossy\": 0.20, \"Musk\": 0.20, \"Orange\": 0.22, \"Oriental\": 0.20, \"Rich\": 0.40,\n",
    "    \"Richer\": 0.22, \"Rose\": 0.20, \"Soft\": 0.20, \"Spicy\": 0.20, \"Tuberose\": 0.20,\n",
    "    \"Valley\": 0.26, \"Violet\": 0.40, \"Water\": 0.20, \"White\": 0.20, \"Woods\": 0.20, \"Woody\": 0.20\n",
    "}\n",
    "\n",
    "def eval_with_fixed_thresholds(clf, X_val_emb, y_val):\n",
    "    \"\"\"확률→고정 임계값 적용→Micro/Sample/Macro F1 반환\"\"\"\n",
    "    y_proba = np.array(clf.predict_proba(X_val_emb))\n",
    "    y_pred = np.zeros_like(y_val)\n",
    "    for i, label in enumerate(mlb.classes_):\n",
    "        thr = fixed_thresholds.get(label, 0.5)\n",
    "        y_pred[:, i] = (y_proba[:, i] >= thr).astype(int)\n",
    "    micro = f1_score(y_val, y_pred, average=\"micro\")\n",
    "    sample = f1_score(y_val, y_pred, average=\"samples\")\n",
    "    macro = f1_score(y_val, y_pred, average=\"macro\")\n",
    "    return micro, sample, macro, y_pred\n",
    "\n",
    "# 탐색 후보 (8개)\n",
    "weight_grid = [(1.3, 0.8), (1.4, 0.8)]\n",
    "lr_c_grid = [1.0, 1.3]\n",
    "xgb_l2_grid = [1.0, 1.5]   # reg_lambda\n",
    "\n",
    "best = None\n",
    "best_clf = None\n",
    "print(\"\\n[Mini Search] Trying tiny grid of (weights, LR.C, XGB.reg_lambda)\")\n",
    "for w in weight_grid:\n",
    "    for c in lr_c_grid:\n",
    "        for l2 in xgb_l2_grid:\n",
    "            # ── 모델 구성 ───────────────────────────────────────────────\n",
    "            logreg = LogisticRegression(max_iter=200, class_weight=\"balanced\", C=c)\n",
    "            xgb = XGBClassifier(\n",
    "                objective=\"binary:logistic\",\n",
    "                eval_metric=\"logloss\",\n",
    "                use_label_encoder=False,\n",
    "                tree_method=\"hist\",\n",
    "                device=\"cuda\" if device == \"cuda\" else \"cpu\",\n",
    "                random_state=SEED,\n",
    "                # 경량/안전 고정값\n",
    "                max_depth=3,\n",
    "                n_estimators=200,\n",
    "                learning_rate=0.1,\n",
    "                subsample=0.9,\n",
    "                colsample_bytree=0.8,\n",
    "                min_child_weight=1,\n",
    "                gamma=0.0,\n",
    "                reg_lambda=l2,\n",
    "            )\n",
    "            ensemble = VotingClassifier(\n",
    "                estimators=[(\"lr\", logreg), (\"xgb\", xgb)],\n",
    "                voting=\"soft\",\n",
    "                weights=list(w),\n",
    "            )\n",
    "            clf_try = OneVsRestClassifier(ensemble, n_jobs=-1)\n",
    "            clf_try.fit(X_train_emb, y_train)\n",
    "\n",
    "            micro, sample, macro, _ = eval_with_fixed_thresholds(clf_try, X_val_emb, y_val)\n",
    "            print(f\"  weights={w}, C={c}, reg_lambda={l2}  →  Micro {micro:.4f} | Sample {sample:.4f} | Macro {macro:.4f}\")\n",
    "\n",
    "            if (best is None) or (micro > best[\"micro\"]):\n",
    "                best = {\"weights\": w, \"C\": c, \"reg_lambda\": l2, \"micro\": micro, \"sample\": sample, \"macro\": macro}\n",
    "                best_clf = clf_try\n",
    "\n",
    "print(\"\\n[Mini Search] Best by Micro-F1:\", best)\n",
    "\n",
    "# 최적 조합 모델 선택\n",
    "clf = best_clf\n",
    "\n",
    "# -------------------------------\n",
    "# 4) 검증 예측 & 임계값 적용 (best 모델)\n",
    "# -------------------------------\n",
    "y_val_proba = np.array(clf.predict_proba(X_val_emb))\n",
    "\n",
    "print(\"\\n[Best Thresholds per label](fixed)\")\n",
    "y_val_pred_opt = np.zeros_like(y_val)\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    thr = fixed_thresholds.get(label, 0.5)\n",
    "    print(f\"{label}: {thr:.2f}\")\n",
    "    y_val_pred_opt[:, i] = (y_val_proba[:, i] >= thr).astype(int)\n",
    "\n",
    "# -------------------------------\n",
    "# 5) 평가\n",
    "# -------------------------------\n",
    "micro_f1 = f1_score(y_val, y_val_pred_opt, average=\"micro\")\n",
    "macro_f1 = f1_score(y_val, y_val_pred_opt, average=\"macro\")\n",
    "sample_f1 = f1_score(y_val, y_val_pred_opt, average=\"samples\")\n",
    "\n",
    "print(\"\\n=== Threshold-based (mini-search best) ===\")\n",
    "print(f\"Micro-F1: {micro_f1:.4f}\")\n",
    "print(f\"Macro-F1: {macro_f1:.4f}\")\n",
    "print(f\"Sample-F1: {sample_f1:.4f}\")\n",
    "\n",
    "print(\"\\n[classification_report @thr]\")\n",
    "print(classification_report(y_val, y_val_pred_opt, target_names=mlb.classes_, zero_division=0))\n",
    "\n",
    "# -------------------------------\n",
    "# 6) 예측 함수\n",
    "# -------------------------------\n",
    "def predict_multilingual(text: str, topk=3, thresholds=None):\n",
    "    emb = encode_texts([text], batch_size=1)\n",
    "    proba = clf.predict_proba(emb)[0]\n",
    "    if thresholds is not None:\n",
    "        pick = [i for i, p in enumerate(proba) if p >= thresholds.get(mlb.classes_[i], 0.5)]\n",
    "        if not pick:\n",
    "            pick = np.argsort(-proba)[:topk]\n",
    "    else:\n",
    "        pick = np.argsort(-proba)[:topk]\n",
    "    return [mlb.classes_[i] for i in pick]\n",
    "\n",
    "print(\"\\n[Example Prediction]\")\n",
    "print(predict_multilingual(\n",
    "    \"바닷가에서 느껴지는 시원하고 약간 달콤한 향이 좋아요\",\n",
    "    topk=TOP_K,\n",
    "    thresholds=fixed_thresholds\n",
    "))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
