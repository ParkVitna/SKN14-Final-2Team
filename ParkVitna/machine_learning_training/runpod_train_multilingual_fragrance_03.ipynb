{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72cb9904-3701-480b-9222-9e98ab9818cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch<3.0,>=2.2 in /usr/local/lib/python3.11/dist-packages (2.8.0.dev20250319+cu128)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.22.0.dev20250319+cu128)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0.dev20250319+cu128)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch<3.0,>=2.2) (77.0.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.2) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.2) (2.1.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting tqdm (from sentence-transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.8.0.dev20250319+cu128)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.12.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch>=1.11.0->sentence-transformers) (77.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m136.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m186.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.9/798.9 kB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, tqdm, threadpoolctl, safetensors, regex, numpy, joblib, hf-xet, scipy, pandas, huggingface-hub, tokenizers, scikit-learn, transformers, sentence-transformers\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.2\n",
      "    Uninstalling numpy-2.1.2:\n",
      "      Successfully uninstalled numpy-2.1.2\n",
      "Successfully installed hf-xet-1.1.9 huggingface-hub-0.34.4 joblib-1.5.2 numpy-2.3.2 pandas-2.3.2 pytz-2025.2 regex-2025.7.34 safetensors-0.6.2 scikit-learn-1.7.1 scipy-1.16.1 sentence-transformers-5.1.0 threadpoolctl-3.6.0 tokenizers-0.21.4 tqdm-4.67.1 transformers-4.55.4 tzdata-2025.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -U \"torch>=2.2,<3.0\" torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!python -m pip install -U sentence-transformers scikit-learn pandas numpy joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34367a0f-ac6a-41f1-9563-01d10ba91589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.4-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.3.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.25.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.16.1)\n",
      "Downloading xgboost-3.0.4-py3-none-manylinux_2_28_x86_64.whl (94.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4952c5ae-2784-44b7-a351-ab8ed8d94e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bf3b63baa6454c806cf7b6bd84f10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8248d5a82ad640b2b046205d017ee6da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad6356fe11d4a4abd001b43f49a6d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634df04b68ff40f4b525aacfcba03f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186b59e51c644f8bad83eefc98874d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ffa867974fd421e965f577c4db3eb27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c825eb70784fe8a0d24cb6bb975e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77c41ea2fd7498faade4f776dc29e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836d93e264484477a5bae47da5e8ab80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2f8fbeef1e4738a448aba693ae779e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Embed] train: (21001, 384) | time: 12.36s\n",
      "[Embed] valid: (5251, 384) | time: 3.05s\n",
      "[Train] OvR-XGBoost: 130.72s\n",
      "\n",
      "[Best Thresholds per label]\n",
      "Amber: 0.24\n",
      "Aromatic: 0.20\n",
      "Blossom: 0.20\n",
      "Bouquet: 0.20\n",
      "Carnation: 0.20\n",
      "Citrus: 0.20\n",
      "Classical: 0.22\n",
      "Crisp: 0.20\n",
      "Dry: 0.20\n",
      "Floral: 0.32\n",
      "Flower: 0.20\n",
      "Fougère: 0.20\n",
      "Fresh: 0.20\n",
      "Fresher: 0.30\n",
      "Fruity: 0.20\n",
      "Gardenia: 0.20\n",
      "Gourmand: 0.20\n",
      "Green: 0.20\n",
      "Iris: 0.46\n",
      "Jasmine: 0.24\n",
      "Lily: 0.26\n",
      "Magnolia: 0.20\n",
      "Mimosa: 0.20\n",
      "Mossy: 0.20\n",
      "Musk: 0.20\n",
      "Orange: 0.48\n",
      "Oriental: 0.20\n",
      "Rich: 0.20\n",
      "Richer: 0.32\n",
      "Rose: 0.22\n",
      "Soft: 0.20\n",
      "Spicy: 0.20\n",
      "Tuberose: 0.20\n",
      "Valley: 0.46\n",
      "Violet: 0.32\n",
      "Water: 0.20\n",
      "White: 0.20\n",
      "Woods: 0.20\n",
      "Woody: 0.20\n",
      "of: 0.46\n",
      "the: 0.46\n",
      "\n",
      "=== Threshold-based ===\n",
      "Micro-F1: 0.4935\n",
      "Macro-F1: 0.2511\n",
      "Sample-F1: 0.4896\n",
      "\n",
      "[classification_report @thr]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Amber       0.42      0.79      0.55      1738\n",
      "    Aromatic       0.41      0.29      0.34       450\n",
      "     Blossom       0.40      0.08      0.13        25\n",
      "     Bouquet       0.40      0.04      0.08        47\n",
      "   Carnation       0.00      0.00      0.00         2\n",
      "      Citrus       0.32      0.41      0.36       981\n",
      "   Classical       0.38      0.60      0.46      1313\n",
      "       Crisp       0.29      0.27      0.28       858\n",
      "         Dry       0.50      0.15      0.24       260\n",
      "      Floral       0.63      0.78      0.70      2141\n",
      "      Flower       0.28      0.05      0.09       329\n",
      "     Fougère       0.41      0.29      0.34       450\n",
      "       Fresh       0.00      0.00      0.00        22\n",
      "     Fresher       0.58      0.93      0.71      2838\n",
      "      Fruity       0.34      0.46      0.39       899\n",
      "    Gardenia       0.00      0.00      0.00         5\n",
      "    Gourmand       0.35      0.14      0.20       326\n",
      "       Green       0.42      0.09      0.15       314\n",
      "        Iris       0.67      0.06      0.10        36\n",
      "     Jasmine       0.91      0.28      0.43        36\n",
      "        Lily       0.60      0.38      0.46         8\n",
      "    Magnolia       0.00      0.00      0.00         4\n",
      "      Mimosa       0.00      0.00      0.00         7\n",
      "       Mossy       0.53      0.04      0.07       251\n",
      "        Musk       0.59      0.12      0.20        82\n",
      "      Orange       1.00      0.04      0.08        25\n",
      "    Oriental       0.00      0.00      0.00         1\n",
      "        Rich       0.00      0.00      0.00        41\n",
      "      Richer       0.58      0.04      0.07       179\n",
      "        Rose       0.68      0.37      0.48       182\n",
      "        Soft       0.40      0.16      0.23       489\n",
      "       Spicy       0.33      0.02      0.04        42\n",
      "    Tuberose       0.80      0.15      0.26        26\n",
      "      Valley       0.67      0.33      0.44         6\n",
      "      Violet       1.00      0.25      0.40         4\n",
      "       Water       0.42      0.20      0.28       166\n",
      "       White       0.28      0.05      0.09       329\n",
      "       Woods       0.35      0.58      0.44      1138\n",
      "       Woody       0.31      0.31      0.31       683\n",
      "          of       0.67      0.33      0.44         6\n",
      "         the       0.67      0.33      0.44         6\n",
      "\n",
      "   micro avg       0.45      0.54      0.49     16745\n",
      "   macro avg       0.43      0.23      0.25     16745\n",
      "weighted avg       0.45      0.54      0.46     16745\n",
      " samples avg       0.47      0.55      0.49     16745\n",
      "\n",
      "\n",
      "=== Top-K-based ===\n",
      "Micro-F1: 0.4790\n",
      "Macro-F1: 0.2003\n",
      "Sample-F1: 0.4772\n",
      "\n",
      "[classification_report @topK]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Amber       0.44      0.70      0.54      1738\n",
      "    Aromatic       0.45      0.16      0.23       450\n",
      "     Blossom       0.00      0.00      0.00        25\n",
      "     Bouquet       0.33      0.02      0.04        47\n",
      "   Carnation       0.00      0.00      0.00         2\n",
      "      Citrus       0.37      0.28      0.32       981\n",
      "   Classical       0.39      0.44      0.41      1313\n",
      "       Crisp       0.32      0.13      0.18       858\n",
      "         Dry       0.60      0.10      0.17       260\n",
      "      Floral       0.61      0.82      0.70      2141\n",
      "      Flower       0.32      0.03      0.06       329\n",
      "     Fougère       0.42      0.23      0.30       450\n",
      "       Fresh       0.00      0.00      0.00        22\n",
      "     Fresher       0.57      0.92      0.71      2838\n",
      "      Fruity       0.38      0.25      0.30       899\n",
      "    Gardenia       0.00      0.00      0.00         5\n",
      "    Gourmand       0.48      0.08      0.13       326\n",
      "       Green       0.41      0.06      0.10       314\n",
      "        Iris       0.67      0.06      0.10        36\n",
      "     Jasmine       0.91      0.28      0.43        36\n",
      "        Lily       0.67      0.25      0.36         8\n",
      "    Magnolia       0.00      0.00      0.00         4\n",
      "      Mimosa       0.00      0.00      0.00         7\n",
      "       Mossy       0.67      0.02      0.03       251\n",
      "        Musk       0.60      0.07      0.13        82\n",
      "      Orange       0.00      0.00      0.00        25\n",
      "    Oriental       0.00      0.00      0.00         1\n",
      "        Rich       0.00      0.00      0.00        41\n",
      "      Richer       0.58      0.04      0.07       179\n",
      "        Rose       0.71      0.34      0.46       182\n",
      "        Soft       0.49      0.09      0.16       489\n",
      "       Spicy       0.33      0.02      0.04        42\n",
      "    Tuberose       0.75      0.12      0.20        26\n",
      "      Valley       1.00      0.17      0.29         6\n",
      "      Violet       0.50      0.25      0.33         4\n",
      "       Water       0.45      0.17      0.25       166\n",
      "       White       0.50      0.01      0.01       329\n",
      "       Woods       0.39      0.41      0.40      1138\n",
      "       Woody       0.37      0.16      0.22       683\n",
      "          of       0.50      0.17      0.25         6\n",
      "         the       1.00      0.17      0.29         6\n",
      "\n",
      "   micro avg       0.49      0.46      0.48     16745\n",
      "   macro avg       0.42      0.17      0.20     16745\n",
      "weighted avg       0.47      0.46      0.42     16745\n",
      " samples avg       0.49      0.47      0.48     16745\n",
      "\n",
      "\n",
      "[Example Prediction]\n",
      "['Floral', 'Fresher', 'Fruity']\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 설치 (Runpod A40 / 로컬)\n",
    "# ============================================\n",
    "# CPU 전용\n",
    "# python -m pip install -U sentence-transformers \"torch>=2.2,<3.0\" scikit-learn pandas numpy joblib xgboost\n",
    "\n",
    "# GPU (CUDA 12.1, Runpod A40)\n",
    "# python -m pip install -U \"torch>=2.2,<3.0\" torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# python -m pip install -U sentence-transformers scikit-learn pandas numpy joblib xgboost\n",
    "# ============================================\n",
    "\n",
    "import os, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -------------------------------\n",
    "# 설정\n",
    "# -------------------------------\n",
    "DATA_CSV = \"perfumes_huggingface.csv\"  # 경로 맞게 수정\n",
    "MODEL_NAME = \"paraphrase-multilingual-MiniLM-L12-v2\"  # 또는 distiluse-base-multilingual-cased-v2\n",
    "TOP_K = 3\n",
    "RARE_MIN_COUNT = 10\n",
    "\n",
    "# -------------------------------\n",
    "# 유틸\n",
    "# -------------------------------\n",
    "def split_labels(s: str):\n",
    "    s = str(s)\n",
    "    for sep in [\",\", \"|\", \"/\", \";\"]:\n",
    "        s = s.replace(sep, \" \")\n",
    "    return [t.strip() for t in s.split() if t.strip()]\n",
    "\n",
    "def encode_with_auto_batch(embedder: SentenceTransformer, texts, init_bs=1024, min_bs=64):\n",
    "    \"\"\"CUDA OOM 시 배치 크기를 줄여가며 안전하게 임베딩\"\"\"\n",
    "    bs = init_bs\n",
    "    Xs = []\n",
    "    i = 0\n",
    "    n = len(texts)\n",
    "    while i < n:\n",
    "        j = min(i + bs, n)\n",
    "        chunk = texts[i:j]\n",
    "        try:\n",
    "            emb = embedder.encode(chunk, batch_size=bs, convert_to_numpy=True, show_progress_bar=False)\n",
    "            Xs.append(emb)\n",
    "            i = j\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e) and bs > min_bs:\n",
    "                torch.cuda.empty_cache()\n",
    "                bs = max(min_bs, bs // 2)\n",
    "                print(f\"[WARN] CUDA OOM → batch_size 축소: {bs}\")\n",
    "                continue\n",
    "            raise\n",
    "    return np.vstack(Xs)\n",
    "\n",
    "# -------------------------------\n",
    "# 1) 데이터 로드 & 전처리\n",
    "# -------------------------------\n",
    "df = pd.read_csv(DATA_CSV, sep=\"|\", engine=\"python\", on_bad_lines=\"skip\")\n",
    "df = df[~df[\"description\"].isna()].copy()\n",
    "df[\"labels\"] = df[\"fragrances\"].apply(split_labels)\n",
    "\n",
    "# 희소 라벨 제거\n",
    "cnt = Counter([l for L in df[\"labels\"] for l in L])\n",
    "rare = {k for k, v in cnt.items() if v <= RARE_MIN_COUNT}\n",
    "df[\"labels\"] = df[\"labels\"].apply(lambda L: [l for l in L if l not in rare])\n",
    "df = df[df[\"labels\"].map(len) > 0].copy()\n",
    "\n",
    "# 타깃 인코딩\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(df[\"labels\"])\n",
    "\n",
    "# -------------------------------\n",
    "# 2) 데이터 분할\n",
    "# -------------------------------\n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
    "    df[\"description\"].tolist(), Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 3) 임베딩\n",
    "# -------------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[Device] {device}\")\n",
    "\n",
    "embedder = SentenceTransformer(MODEL_NAME, device=device)\n",
    "init_bs = 1024 if device == \"cuda\" else 128\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "X_train = encode_with_auto_batch(embedder, X_train_text, init_bs=init_bs, min_bs=64)\n",
    "t1 = time.perf_counter()\n",
    "print(f\"[Embed] train: {X_train.shape} | time: {t1 - t0:.2f}s\")\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "X_val = encode_with_auto_batch(embedder, X_val_text, init_bs=init_bs, min_bs=64)\n",
    "t1 = time.perf_counter()\n",
    "print(f\"[Embed] valid: {X_val.shape} | time: {t1 - t0:.2f}s\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4) XGBoost OvR 학습\n",
    "# -------------------------------\n",
    "clf = OneVsRestClassifier(\n",
    "    xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        use_label_encoder=False,\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\"\n",
    "    )\n",
    ")\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "clf.fit(X_train, y_train)\n",
    "t1 = time.perf_counter()\n",
    "print(f\"[Train] OvR-XGBoost: {t1 - t0:.2f}s\")\n",
    "\n",
    "y_val_proba = clf.predict_proba(X_val)\n",
    "\n",
    "# -------------------------------\n",
    "# 5) 라벨별 Threshold 최적화 (0.2~0.5 Grid Search)\n",
    "# -------------------------------\n",
    "thresholds = {}\n",
    "y_val_pred_opt = np.zeros_like(y_val)\n",
    "\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    best_thr, best_f1 = 0.5, -1\n",
    "    for thr in np.linspace(0.2, 0.5, 16):  # 0.2~0.5 구간\n",
    "        pred = (y_val_proba[:, i] >= thr).astype(int)\n",
    "        f1 = f1_score(y_val[:, i], pred, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_thr, best_f1 = thr, f1\n",
    "    thresholds[label] = best_thr\n",
    "    y_val_pred_opt[:, i] = (y_val_proba[:, i] >= best_thr).astype(int)\n",
    "\n",
    "print(\"\\n[Best Thresholds per label]\")\n",
    "for k, v in thresholds.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 6) 평가 (Threshold-based + Top-K-based)\n",
    "# -------------------------------\n",
    "print(\"\\n=== Threshold-based ===\")\n",
    "print(f\"Micro-F1: {f1_score(y_val, y_val_pred_opt, average='micro'):.4f}\")\n",
    "print(f\"Macro-F1: {f1_score(y_val, y_val_pred_opt, average='macro'):.4f}\")\n",
    "print(f\"Sample-F1: {f1_score(y_val, y_val_pred_opt, average='samples'):.4f}\")\n",
    "print(\"\\n[classification_report @thr]\")\n",
    "print(classification_report(y_val, y_val_pred_opt, target_names=mlb.classes_, zero_division=0))\n",
    "\n",
    "# Top-K\n",
    "top_idx = np.argsort(-y_val_proba, axis=1)[:, :TOP_K]\n",
    "y_val_topk = np.zeros_like(y_val_proba, dtype=int)\n",
    "for i, idxs in enumerate(top_idx):\n",
    "    y_val_topk[i, idxs] = 1\n",
    "\n",
    "print(\"\\n=== Top-K-based ===\")\n",
    "print(f\"Micro-F1: {f1_score(y_val, y_val_topk, average='micro'):.4f}\")\n",
    "print(f\"Macro-F1: {f1_score(y_val, y_val_topk, average='macro'):.4f}\")\n",
    "print(f\"Sample-F1: {f1_score(y_val, y_val_topk, average='samples'):.4f}\")\n",
    "print(\"\\n[classification_report @topK]\")\n",
    "print(classification_report(y_val, y_val_topk, target_names=mlb.classes_, zero_division=0))\n",
    "\n",
    "# -------------------------------\n",
    "# 7) 예측 함수\n",
    "# -------------------------------\n",
    "def predict_multilingual(text: str, topk=3, thresholds=None):\n",
    "    v = encode_with_auto_batch(embedder, [text], init_bs=64 if device==\"cpu\" else 256, min_bs=32)\n",
    "    proba = clf.predict_proba(v)[0]\n",
    "\n",
    "    if thresholds is not None:  # 라벨별 threshold 적용\n",
    "        pick = [i for i, p in enumerate(proba) if p >= thresholds.get(mlb.classes_[i], 0.5)]\n",
    "        if not pick:  # 아무 라벨도 안 나오면 Top-K fallback\n",
    "            pick = np.argsort(-proba)[:topk]\n",
    "    else:  # Top-K\n",
    "        pick = np.argsort(-proba)[:topk]\n",
    "\n",
    "    return [mlb.classes_[i] for i in pick]\n",
    "\n",
    "# -------------------------------\n",
    "# 예시 실행\n",
    "# -------------------------------\n",
    "print(\"\\n[Example Prediction]\")\n",
    "print(predict_multilingual(\"바닷가에서 느껴지는 시원하고 약간 달콤한 향이 좋아요\", topk=3, thresholds=thresholds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4717c8b5-77f0-41b2-8f93-ca7a9ac58237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Example Prediction]\n",
      "['Fresher', 'Amber', 'Floral']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[Example Prediction]\")\n",
    "print(predict_multilingual(\"깨끗하게 빨래하고 말린 상쾌한 향\", topk=3, thresholds=thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4f283a-1ee2-4b2f-b0ba-00f3189c40ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Example Prediction]\n",
      "['Fresher', 'Woods', 'Classical']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[Example Prediction]\")\n",
    "print(predict_multilingual(\"바다향\", topk=3, thresholds=thresholds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
