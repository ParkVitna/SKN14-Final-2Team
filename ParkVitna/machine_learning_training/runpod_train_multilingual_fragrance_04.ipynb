{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72cb9904-3701-480b-9222-9e98ab9818cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch<3.0,>=2.2 in /usr/local/lib/python3.11/dist-packages (2.8.0.dev20250319+cu128)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.22.0.dev20250319+cu128)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0.dev20250319+cu128)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch<3.0,>=2.2) (77.0.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.2) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.2) (2.1.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting tqdm (from sentence-transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.8.0.dev20250319+cu128)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.12.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch>=1.11.0->sentence-transformers) (77.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m136.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m186.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.9/798.9 kB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, tqdm, threadpoolctl, safetensors, regex, numpy, joblib, hf-xet, scipy, pandas, huggingface-hub, tokenizers, scikit-learn, transformers, sentence-transformers\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.2\n",
      "    Uninstalling numpy-2.1.2:\n",
      "      Successfully uninstalled numpy-2.1.2\n",
      "Successfully installed hf-xet-1.1.9 huggingface-hub-0.34.4 joblib-1.5.2 numpy-2.3.2 pandas-2.3.2 pytz-2025.2 regex-2025.7.34 safetensors-0.6.2 scikit-learn-1.7.1 scipy-1.16.1 sentence-transformers-5.1.0 threadpoolctl-3.6.0 tokenizers-0.21.4 tqdm-4.67.1 transformers-4.55.4 tzdata-2025.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -U \"torch>=2.2,<3.0\" torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!python -m pip install -U sentence-transformers scikit-learn pandas numpy joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34367a0f-ac6a-41f1-9563-01d10ba91589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.4-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.3.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.25.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.16.1)\n",
      "Downloading xgboost-3.0.4-py3-none-manylinux_2_28_x86_64.whl (94.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4952c5ae-2784-44b7-a351-ab8ed8d94e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] cuda\n",
      "[Train] OvR-XGBoost (1000 trees): 511.22s\n",
      "\n",
      "=== Threshold-based ===\n",
      "Micro-F1: 0.4910\n",
      "Macro-F1: 0.2250\n",
      "Sample-F1: 0.4801\n",
      "\n",
      "[classification_report @thr]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Amber       0.46      0.68      0.55      1738\n",
      "    Aromatic       0.48      0.22      0.30       450\n",
      "     Blossom       0.67      0.08      0.14        25\n",
      "     Bouquet       0.50      0.06      0.11        47\n",
      "   Carnation       0.00      0.00      0.00         2\n",
      "      Citrus       0.41      0.30      0.35       981\n",
      "   Classical       0.42      0.46      0.44      1313\n",
      "       Crisp       0.36      0.16      0.22       858\n",
      "         Dry       0.69      0.13      0.21       260\n",
      "      Floral       0.67      0.75      0.71      2141\n",
      "      Flower       0.33      0.02      0.04       329\n",
      "     Fougère       0.48      0.22      0.30       450\n",
      "       Fresh       0.00      0.00      0.00        22\n",
      "     Fresher       0.59      0.91      0.71      2838\n",
      "      Fruity       0.42      0.33      0.37       899\n",
      "    Gardenia       0.00      0.00      0.00         5\n",
      "    Gourmand       0.47      0.10      0.17       326\n",
      "       Green       0.44      0.06      0.11       314\n",
      "        Iris       0.50      0.03      0.05        36\n",
      "     Jasmine       0.91      0.28      0.43        36\n",
      "        Lily       0.60      0.38      0.46         8\n",
      "    Magnolia       0.00      0.00      0.00         4\n",
      "      Mimosa       0.00      0.00      0.00         7\n",
      "       Mossy       0.78      0.03      0.05       251\n",
      "        Musk       0.67      0.12      0.21        82\n",
      "      Orange       0.67      0.08      0.14        25\n",
      "    Oriental       0.00      0.00      0.00         1\n",
      "        Rich       0.00      0.00      0.00        41\n",
      "      Richer       0.54      0.04      0.07       179\n",
      "        Rose       0.72      0.36      0.48       182\n",
      "        Soft       0.57      0.11      0.18       489\n",
      "       Spicy       0.33      0.02      0.04        42\n",
      "    Tuberose       0.83      0.19      0.31        26\n",
      "      Valley       0.50      0.17      0.25         6\n",
      "      Violet       0.50      0.25      0.33         4\n",
      "       Water       0.47      0.19      0.27       166\n",
      "       White       0.33      0.02      0.04       329\n",
      "       Woods       0.40      0.44      0.42      1138\n",
      "       Woody       0.35      0.18      0.24       683\n",
      "          of       0.50      0.17      0.25         6\n",
      "         the       0.50      0.17      0.25         6\n",
      "\n",
      "   micro avg       0.52      0.47      0.49     16745\n",
      "   macro avg       0.44      0.19      0.22     16745\n",
      "weighted avg       0.50      0.47      0.44     16745\n",
      " samples avg       0.53      0.48      0.48     16745\n",
      "\n",
      "\n",
      "=== Top-K-based ===\n",
      "Micro-F1: 0.4874\n",
      "Macro-F1: 0.2107\n",
      "Sample-F1: 0.4859\n",
      "\n",
      "[classification_report @topK]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Amber       0.44      0.73      0.55      1738\n",
      "    Aromatic       0.50      0.18      0.26       450\n",
      "     Blossom       0.00      0.00      0.00        25\n",
      "     Bouquet       0.50      0.04      0.08        47\n",
      "   Carnation       0.00      0.00      0.00         2\n",
      "      Citrus       0.41      0.29      0.34       981\n",
      "   Classical       0.41      0.46      0.43      1313\n",
      "       Crisp       0.33      0.12      0.17       858\n",
      "         Dry       0.69      0.10      0.18       260\n",
      "      Floral       0.60      0.83      0.70      2141\n",
      "      Flower       0.26      0.02      0.03       329\n",
      "     Fougère       0.46      0.25      0.33       450\n",
      "       Fresh       0.00      0.00      0.00        22\n",
      "     Fresher       0.57      0.93      0.71      2838\n",
      "      Fruity       0.42      0.25      0.32       899\n",
      "    Gardenia       0.00      0.00      0.00         5\n",
      "    Gourmand       0.53      0.08      0.14       326\n",
      "       Green       0.41      0.07      0.12       314\n",
      "        Iris       0.00      0.00      0.00        36\n",
      "     Jasmine       0.92      0.31      0.46        36\n",
      "        Lily       0.67      0.25      0.36         8\n",
      "    Magnolia       0.00      0.00      0.00         4\n",
      "      Mimosa       0.00      0.00      0.00         7\n",
      "       Mossy       0.71      0.02      0.04       251\n",
      "        Musk       0.80      0.10      0.17        82\n",
      "      Orange       1.00      0.08      0.15        25\n",
      "    Oriental       0.00      0.00      0.00         1\n",
      "        Rich       0.00      0.00      0.00        41\n",
      "      Richer       0.54      0.04      0.07       179\n",
      "        Rose       0.71      0.35      0.46       182\n",
      "        Soft       0.57      0.08      0.14       489\n",
      "       Spicy       0.33      0.02      0.04        42\n",
      "    Tuberose       0.83      0.19      0.31        26\n",
      "      Valley       1.00      0.17      0.29         6\n",
      "      Violet       0.50      0.25      0.33         4\n",
      "       Water       0.53      0.19      0.28       166\n",
      "       White       0.50      0.01      0.01       329\n",
      "       Woods       0.40      0.42      0.41      1138\n",
      "       Woody       0.39      0.15      0.21       683\n",
      "          of       0.50      0.17      0.25         6\n",
      "         the       1.00      0.17      0.29         6\n",
      "\n",
      "   micro avg       0.50      0.47      0.49     16745\n",
      "   macro avg       0.45      0.18      0.21     16745\n",
      "weighted avg       0.49      0.47      0.42     16745\n",
      " samples avg       0.50      0.48      0.49     16745\n",
      "\n",
      "\n",
      "[Example Prediction]\n",
      "['Amber', 'Floral', 'Fresher']\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# XGBoost OvR + 라벨별 최적 threshold 적용 + epoch 확장\n",
    "# ============================================\n",
    "\n",
    "import os, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -------------------------------\n",
    "# 설정\n",
    "# -------------------------------\n",
    "DATA_CSV = \"perfumes_huggingface.csv\"  # 경로 맞게 수정\n",
    "MODEL_NAME = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "TOP_K = 3\n",
    "RARE_MIN_COUNT = 10\n",
    "\n",
    "# 라벨별 최적 threshold (앞에서 찾은 값 복붙)\n",
    "thresholds = {\n",
    "    \"Amber\":0.24,\"Aromatic\":0.20,\"Blossom\":0.20,\"Bouquet\":0.20,\"Carnation\":0.20,\"Citrus\":0.20,\n",
    "    \"Classical\":0.22,\"Crisp\":0.20,\"Dry\":0.20,\"Floral\":0.32,\"Flower\":0.20,\"Fougère\":0.20,\n",
    "    \"Fresh\":0.20,\"Fresher\":0.30,\"Fruity\":0.20,\"Gardenia\":0.20,\"Gourmand\":0.20,\"Green\":0.20,\n",
    "    \"Iris\":0.46,\"Jasmine\":0.24,\"Lily\":0.26,\"Magnolia\":0.20,\"Mimosa\":0.20,\"Mossy\":0.20,\"Musk\":0.20,\n",
    "    \"Orange\":0.48,\"Oriental\":0.20,\"Rich\":0.20,\"Richer\":0.32,\"Rose\":0.22,\"Soft\":0.20,\"Spicy\":0.20,\n",
    "    \"Tuberose\":0.20,\"Valley\":0.46,\"Violet\":0.32,\"Water\":0.20,\"White\":0.20,\"Woods\":0.20,\"Woody\":0.20,\n",
    "    \"of\":0.46,\"the\":0.46\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# 유틸\n",
    "# -------------------------------\n",
    "def split_labels(s: str):\n",
    "    s = str(s)\n",
    "    for sep in [\",\", \"|\", \"/\", \";\"]:\n",
    "        s = s.replace(sep, \" \")\n",
    "    return [t.strip() for t in s.split() if t.strip()]\n",
    "\n",
    "def encode_with_auto_batch(embedder: SentenceTransformer, texts, init_bs=1024, min_bs=64):\n",
    "    \"\"\"CUDA OOM 시 배치 크기를 줄여가며 안전하게 임베딩\"\"\"\n",
    "    bs = init_bs\n",
    "    Xs = []\n",
    "    i = 0\n",
    "    n = len(texts)\n",
    "    while i < n:\n",
    "        j = min(i + bs, n)\n",
    "        chunk = texts[i:j]\n",
    "        try:\n",
    "            emb = embedder.encode(chunk, batch_size=bs, convert_to_numpy=True, show_progress_bar=False)\n",
    "            Xs.append(emb)\n",
    "            i = j\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e) and bs > min_bs:\n",
    "                torch.cuda.empty_cache()\n",
    "                bs = max(min_bs, bs // 2)\n",
    "                print(f\"[WARN] CUDA OOM → batch_size 축소: {bs}\")\n",
    "                continue\n",
    "            raise\n",
    "    return np.vstack(Xs)\n",
    "\n",
    "# -------------------------------\n",
    "# 1) 데이터 로드 & 전처리\n",
    "# -------------------------------\n",
    "df = pd.read_csv(DATA_CSV, sep=\"|\", engine=\"python\", on_bad_lines=\"skip\")\n",
    "df = df[~df[\"description\"].isna()].copy()\n",
    "df[\"labels\"] = df[\"fragrances\"].apply(split_labels)\n",
    "\n",
    "# 희소 라벨 제거\n",
    "cnt = Counter([l for L in df[\"labels\"] for l in L])\n",
    "rare = {k for k, v in cnt.items() if v <= RARE_MIN_COUNT}\n",
    "df[\"labels\"] = df[\"labels\"].apply(lambda L: [l for l in L if l not in rare])\n",
    "df = df[df[\"labels\"].map(len) > 0].copy()\n",
    "\n",
    "# 타깃 인코딩\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(df[\"labels\"])\n",
    "\n",
    "# -------------------------------\n",
    "# 2) 데이터 분할\n",
    "# -------------------------------\n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
    "    df[\"description\"].tolist(), Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 3) 임베딩\n",
    "# -------------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[Device] {device}\")\n",
    "\n",
    "embedder = SentenceTransformer(MODEL_NAME, device=device)\n",
    "init_bs = 1024 if device == \"cuda\" else 128\n",
    "\n",
    "X_train = encode_with_auto_batch(embedder, X_train_text, init_bs=init_bs, min_bs=64)\n",
    "X_val = encode_with_auto_batch(embedder, X_val_text, init_bs=init_bs, min_bs=64)\n",
    "\n",
    "# -------------------------------\n",
    "# 4) XGBoost OvR 학습 (epoch = n_estimators 확장)\n",
    "# -------------------------------\n",
    "clf = OneVsRestClassifier(\n",
    "    xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        use_label_encoder=False,\n",
    "        n_estimators=1000,   # 🔥 epoch 개념 (200 → 1000으로 늘림)\n",
    "        learning_rate=0.05,  # epoch 늘렸으니 lr 살짝 줄임\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\"\n",
    "    )\n",
    ")\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "clf.fit(X_train, y_train)\n",
    "t1 = time.perf_counter()\n",
    "print(f\"[Train] OvR-XGBoost (1000 trees): {t1 - t0:.2f}s\")\n",
    "\n",
    "y_val_proba = clf.predict_proba(X_val)\n",
    "\n",
    "# -------------------------------\n",
    "# 5) 평가 (Threshold-based + Top-K-based)\n",
    "# -------------------------------\n",
    "# Threshold 기반\n",
    "y_val_thr = np.zeros_like(y_val)\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    thr = thresholds.get(label, 0.5)\n",
    "    y_val_thr[:, i] = (y_val_proba[:, i] >= thr).astype(int)\n",
    "\n",
    "print(\"\\n=== Threshold-based ===\")\n",
    "print(f\"Micro-F1: {f1_score(y_val, y_val_thr, average='micro'):.4f}\")\n",
    "print(f\"Macro-F1: {f1_score(y_val, y_val_thr, average='macro'):.4f}\")\n",
    "print(f\"Sample-F1: {f1_score(y_val, y_val_thr, average='samples'):.4f}\")\n",
    "print(\"\\n[classification_report @thr]\")\n",
    "print(classification_report(y_val, y_val_thr, target_names=mlb.classes_, zero_division=0))\n",
    "\n",
    "# Top-K 기반\n",
    "top_idx = np.argsort(-y_val_proba, axis=1)[:, :TOP_K]\n",
    "y_val_topk = np.zeros_like(y_val_proba, dtype=int)\n",
    "for i, idxs in enumerate(top_idx):\n",
    "    y_val_topk[i, idxs] = 1\n",
    "\n",
    "print(\"\\n=== Top-K-based ===\")\n",
    "print(f\"Micro-F1: {f1_score(y_val, y_val_topk, average='micro'):.4f}\")\n",
    "print(f\"Macro-F1: {f1_score(y_val, y_val_topk, average='macro'):.4f}\")\n",
    "print(f\"Sample-F1: {f1_score(y_val, y_val_topk, average='samples'):.4f}\")\n",
    "print(\"\\n[classification_report @topK]\")\n",
    "print(classification_report(y_val, y_val_topk, target_names=mlb.classes_, zero_division=0))\n",
    "\n",
    "# -------------------------------\n",
    "# 6) 예측 함수\n",
    "# -------------------------------\n",
    "def predict_multilingual(text: str, topk=3, thresholds=None):\n",
    "    v = encode_with_auto_batch(embedder, [text], init_bs=64 if device==\"cpu\" else 256, min_bs=32)\n",
    "    proba = clf.predict_proba(v)[0]\n",
    "\n",
    "    if thresholds is not None:  # 라벨별 threshold 적용\n",
    "        pick = [i for i, p in enumerate(proba) if p >= thresholds.get(mlb.classes_[i], 0.5)]\n",
    "        if not pick:  # 아무 라벨도 안 나오면 Top-K fallback\n",
    "            pick = np.argsort(-proba)[:topk]\n",
    "    else:  # Top-K\n",
    "        pick = np.argsort(-proba)[:topk]\n",
    "\n",
    "    return [mlb.classes_[i] for i in pick]\n",
    "\n",
    "# -------------------------------\n",
    "# 예시 실행\n",
    "# -------------------------------\n",
    "print(\"\\n[Example Prediction]\")\n",
    "print(predict_multilingual(\"바닷가에서 느껴지는 시원하고 약간 달콤한 향이 좋아요\", topk=3, thresholds=thresholds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4717c8b5-77f0-41b2-8f93-ca7a9ac58237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Example Prediction]\n",
      "['Fresher', 'Amber', 'Floral']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[Example Prediction]\")\n",
    "print(predict_multilingual(\"깨끗하게 빨래하고 말린 상쾌한 향\", topk=3, thresholds=thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4f283a-1ee2-4b2f-b0ba-00f3189c40ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Example Prediction]\n",
      "['Fresher', 'Woods', 'Classical']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[Example Prediction]\")\n",
    "print(predict_multilingual(\"바다향\", topk=3, thresholds=thresholds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
