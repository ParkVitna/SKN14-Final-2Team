{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af1e9637-e5e0-4c20-babf-bc9b4d735c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch<3.0,>=2.2 in /usr/local/lib/python3.11/dist-packages (2.8.0.dev20250319+cu128)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.22.0.dev20250319+cu128)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0.dev20250319+cu128)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.2) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch<3.0,>=2.2) (77.0.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.2) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.2) (2.1.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting tqdm (from sentence-transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.8.0.dev20250319+cu128)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch>=1.11.0->sentence-transformers) (77.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.9/798.9 kB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, tqdm, threadpoolctl, safetensors, regex, numpy, joblib, hf-xet, scipy, pandas, huggingface-hub, tokenizers, scikit-learn, transformers, sentence-transformers\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.2\n",
      "    Uninstalling numpy-2.1.2:\n",
      "      Successfully uninstalled numpy-2.1.2\n",
      "Successfully installed hf-xet-1.1.9 huggingface-hub-0.34.4 joblib-1.5.2 numpy-2.3.2 pandas-2.3.2 pytz-2025.2 regex-2025.7.34 safetensors-0.6.2 scikit-learn-1.7.1 scipy-1.16.1 sentence-transformers-5.1.0 threadpoolctl-3.6.0 tokenizers-0.21.4 tqdm-4.67.1 transformers-4.55.4 tzdata-2025.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 설치 (Runpod A40 / 로컬)\n",
    "# ============================================\n",
    "\n",
    "# CPU 전용\n",
    "# python -m pip install -U \"torch>=2.2,<3.0\" scikit-learn pandas numpy joblib sentence-transformers transformers\n",
    "\n",
    "# GPU (CUDA 12.1, Runpod A40)\n",
    "!python -m pip install -U \"torch>=2.2,<3.0\" torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!python -m pip install -U scikit-learn pandas numpy joblib sentence-transformers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3839174a-9050-4063-8774-88ef87217353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.55.4\n",
      "Uninstalling transformers-4.55.4:\n",
      "  Successfully uninstalled transformers-4.55.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: huggingface-hub 0.34.4\n",
      "Uninstalling huggingface-hub-0.34.4:\n",
      "  Successfully uninstalled huggingface-hub-0.34.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: tokenizers 0.21.4\n",
      "Uninstalling tokenizers-0.21.4:\n",
      "  Successfully uninstalled tokenizers-0.21.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformers\n",
      "  Downloading transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m239.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m377.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m482.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.34.4 tokenizers-0.21.4 transformers-4.55.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall transformers -y\n",
    "!pip uninstall huggingface-hub -y\n",
    "!pip uninstall tokenizers -y\n",
    "\n",
    "!pip install --no-cache-dir transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a501d1e-b38b-459d-abfb-825cb9b71822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d42cd028ba4a8cb2f9e6d49f0d1ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d321810218764ae5bf8e90634e0ae86e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74cd10ca1ae84d29bccb5f8c50ca0b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b789c182782a46eda203b20ce5bf0b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb5f21018924ef4b5bff4d8a44adade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.5304\n",
      "Epoch 2 | Train Loss: 0.3349\n",
      "Epoch 3 | Train Loss: 0.2632\n",
      "Epoch 4 | Train Loss: 0.2413\n",
      "Epoch 5 | Train Loss: 0.2331\n",
      "Epoch 6 | Train Loss: 0.2292\n",
      "Epoch 7 | Train Loss: 0.2271\n",
      "Epoch 8 | Train Loss: 0.2257\n",
      "\n",
      "[Best Thresholds per label]\n",
      "Amber: 0.20\n",
      "Aromatic: 0.20\n",
      "Blossom: 0.20\n",
      "Bouquet: 0.20\n",
      "Citrus: 0.20\n",
      "Classical: 0.20\n",
      "Crisp: 0.20\n",
      "Dry: 0.20\n",
      "Floral: 0.40\n",
      "Flower: 0.20\n",
      "Fougère: 0.20\n",
      "Fresh: 0.20\n",
      "Fresher: 0.20\n",
      "Fruity: 0.20\n",
      "Gourmand: 0.20\n",
      "Green: 0.20\n",
      "Iris: 0.20\n",
      "Jasmine: 0.20\n",
      "Lily: 0.20\n",
      "Mossy: 0.20\n",
      "Musk: 0.20\n",
      "Orange: 0.20\n",
      "Rich: 0.20\n",
      "Richer: 0.20\n",
      "Rose: 0.20\n",
      "Soft: 0.20\n",
      "Spicy: 0.20\n",
      "Tuberose: 0.20\n",
      "Valley: 0.20\n",
      "Violet: 0.20\n",
      "Water: 0.20\n",
      "White: 0.20\n",
      "Woods: 0.20\n",
      "Woody: 0.20\n",
      "of: 0.20\n",
      "the: 0.20\n",
      "\n",
      "=== Threshold-based ===\n",
      "Micro-F1: 0.4230\n",
      "Macro-F1: 0.0704\n",
      "Sample-F1: 0.4225\n",
      "\n",
      "[classification_report @thr]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Amber       0.35      1.00      0.52       316\n",
      "    Aromatic       0.00      0.00      0.00        83\n",
      "     Blossom       0.00      0.00      0.00         6\n",
      "     Bouquet       0.00      0.00      0.00         8\n",
      "      Citrus       0.00      0.00      0.00       156\n",
      "   Classical       0.23      1.00      0.37       209\n",
      "       Crisp       0.00      0.00      0.00       153\n",
      "         Dry       0.00      0.00      0.00        45\n",
      "      Floral       0.46      0.92      0.61       373\n",
      "      Flower       0.00      0.00      0.00        73\n",
      "     Fougère       0.00      0.00      0.00        83\n",
      "       Fresh       0.00      0.00      0.00         6\n",
      "     Fresher       0.56      1.00      0.72       505\n",
      "      Fruity       0.00      0.00      0.00       157\n",
      "    Gourmand       0.00      0.00      0.00        63\n",
      "       Green       0.00      0.00      0.00        54\n",
      "        Iris       0.00      0.00      0.00         5\n",
      "     Jasmine       0.00      0.00      0.00         2\n",
      "        Lily       0.00      0.00      0.00         2\n",
      "       Mossy       0.00      0.00      0.00        30\n",
      "        Musk       0.00      0.00      0.00         9\n",
      "      Orange       0.00      0.00      0.00         6\n",
      "        Rich       0.00      0.00      0.00         6\n",
      "      Richer       0.00      0.00      0.00        27\n",
      "        Rose       0.00      0.00      0.00        27\n",
      "        Soft       0.00      0.00      0.00        84\n",
      "       Spicy       0.00      0.00      0.00        11\n",
      "    Tuberose       0.00      0.00      0.00         4\n",
      "      Valley       0.00      0.00      0.00         2\n",
      "      Violet       0.00      0.00      0.00         2\n",
      "       Water       0.00      0.00      0.00        40\n",
      "       White       0.00      0.00      0.00        73\n",
      "       Woods       0.19      1.00      0.32       171\n",
      "       Woody       0.00      0.00      0.00       128\n",
      "          of       0.00      0.00      0.00         2\n",
      "         the       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.35      0.53      0.42      2923\n",
      "   macro avg       0.05      0.14      0.07      2923\n",
      "weighted avg       0.22      0.53      0.30      2923\n",
      " samples avg       0.35      0.55      0.42      2923\n",
      "\n",
      "\n",
      "=== Top-K-based ===\n",
      "Micro-F1: 0.4233\n",
      "Macro-F1: 0.0505\n",
      "Sample-F1: 0.4122\n",
      "\n",
      "[Example Prediction]\n",
      "['Amber', 'Classical', 'Floral', 'Fresher', 'Woods']\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# MiniLM 임베딩 + AutoModelForSequenceClassification + Threshold 최적화 + Epoch 8\n",
    "# ============================================\n",
    "\n",
    "import os, time, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# -------------------------------\n",
    "# 설정\n",
    "# -------------------------------\n",
    "DATA_CSV = \"perfumes_huggingface.csv\"\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "TOP_K = 3\n",
    "RARE_MIN_COUNT = 7\n",
    "MAX_LEN = 384\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 8\n",
    "LR = 1e-5\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[Device] {device}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 유틸 함수\n",
    "# -------------------------------\n",
    "def split_labels(s: str):\n",
    "    s = str(s)\n",
    "    for sep in [\",\", \"|\", \"/\", \";\"]:\n",
    "        s = s.replace(sep, \" \")\n",
    "    return [t.strip() for t in s.split() if t.strip()]\n",
    "\n",
    "# ✨ 전처리 함수 (불용어 단어 단위 매칭)\n",
    "def preprocess_text(text: str, brand_list=None):\n",
    "    text = str(text).lower()\n",
    "\n",
    "    # 브랜드명 제거\n",
    "    if brand_list:\n",
    "        for brand in brand_list:\n",
    "            text = re.sub(rf\"\\b{re.escape(brand.lower())}\\b\", \" \", text)\n",
    "\n",
    "    # 불필요한 특수문자 제거\n",
    "    text = re.sub(r\"[^a-zA-Z가-힣0-9\\s]\", \" \", text)\n",
    "\n",
    "    # 불용어 리스트\n",
    "    stopwords = {\n",
    "        # 관사\n",
    "        \"the\", \"a\", \"an\",\n",
    "    \n",
    "        # 접속사\n",
    "        \"and\", \"or\", \"but\", \"if\", \"while\", \"though\",\n",
    "    \n",
    "        # 전치사\n",
    "        \"of\", \"to\", \"in\", \"for\", \"on\", \"at\", \"by\", \"from\", \"into\", \"onto\",\n",
    "        \"with\", \"about\", \"over\", \"under\", \"between\", \"through\", \"without\",\n",
    "    \n",
    "        # 대명사\n",
    "        \"i\", \"me\", \"my\", \"mine\", \"you\", \"your\", \"yours\",\n",
    "        \"he\", \"him\", \"his\", \"she\", \"her\", \"hers\",\n",
    "        \"it\", \"its\", \"we\", \"us\", \"our\", \"ours\",\n",
    "        \"they\", \"them\", \"their\", \"theirs\",\n",
    "    \n",
    "        # 기타 불필요 단어\n",
    "        \"is\", \"am\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
    "        \"do\", \"does\", \"did\", \"done\",\n",
    "        \"have\", \"has\", \"had\", \"having\",\n",
    "        \"will\", \"would\", \"shall\", \"should\", \"can\", \"could\", \"may\", \"might\", \"must\",\n",
    "    \n",
    "        # 한정사 / 지시어\n",
    "        \"this\", \"that\", \"these\", \"those\", \"such\", \"some\", \"any\", \"each\", \"every\",\n",
    "        \"other\", \"another\", \"all\", \"both\", \"few\", \"many\", \"more\", \"most\", \"much\", \"no\", \"nor\"\n",
    "    }\n",
    "\n",
    "    # 불용어 제거 (단어 단위 매칭)\n",
    "    for sw in stopwords:\n",
    "        text = re.sub(rf\"\\b{re.escape(sw)}\\b\", \" \", text)\n",
    "\n",
    "    # 다중 공백 정리\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# -------------------------------\n",
    "# 1) 데이터 로드 & 전처리\n",
    "# -------------------------------\n",
    "df = pd.read_csv(DATA_CSV, sep=\"|\", engine=\"python\", on_bad_lines=\"skip\")\n",
    "df = df[~df[\"description\"].isna()].copy()\n",
    "df[\"labels\"] = df[\"fragrances\"].apply(split_labels)\n",
    "\n",
    "# 브랜드명 리스트 자동 추출\n",
    "brand_list = []\n",
    "if \"brand\" in df.columns:\n",
    "    brand_list = df[\"brand\"].dropna().unique().tolist()\n",
    "else:\n",
    "    brand_list = []\n",
    "\n",
    "# description 전처리 적용\n",
    "df[\"description\"] = df[\"description\"].apply(lambda x: preprocess_text(x, brand_list))\n",
    "\n",
    "# 희소 라벨 제거\n",
    "cnt = Counter([l for L in df[\"labels\"] for l in L])\n",
    "rare = {k for k, v in cnt.items() if v <= RARE_MIN_COUNT}\n",
    "df[\"labels\"] = df[\"labels\"].apply(lambda L: [l for l in L if l not in rare])\n",
    "df = df[df[\"labels\"].map(len) > 0].copy()\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(df[\"labels\"])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
    "    df[\"description\"].tolist(), Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 2) 토크나이저 및 데이터셋\n",
    "# -------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class PerfumeDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = PerfumeDataset(X_train_text, y_train)\n",
    "val_dataset = PerfumeDataset(X_val_text, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# -------------------------------\n",
    "# 3) 모델 & 옵티마이저\n",
    "# -------------------------------\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=Y.shape[1],\n",
    "    problem_type=\"multi_label_classification\"\n",
    ").to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "\n",
    "# -------------------------------\n",
    "# 4) 학습 루프\n",
    "# -------------------------------\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5) 검증 예측 & Threshold 최적화\n",
    "# -------------------------------\n",
    "model.eval()\n",
    "all_logits, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "        labels = batch[\"labels\"].cpu().numpy()\n",
    "        outputs = model(**inputs).logits.cpu().numpy()\n",
    "        all_logits.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "y_val_proba = torch.sigmoid(torch.tensor(np.vstack(all_logits))).numpy()\n",
    "y_val = np.vstack(all_labels)\n",
    "\n",
    "thresholds = {}\n",
    "y_val_pred_opt = np.zeros_like(y_val)\n",
    "\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    best_thr, best_f1 = 0.5, -1\n",
    "    for thr in np.linspace(0.2, 0.5, 16):\n",
    "        pred = (y_val_proba[:, i] >= thr).astype(int)\n",
    "        f1 = f1_score(y_val[:, i], pred, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_thr, best_f1 = thr, f1\n",
    "    thresholds[label] = best_thr\n",
    "    y_val_pred_opt[:, i] = (y_val_proba[:, i] >= best_thr).astype(int)\n",
    "\n",
    "print(\"\\n[Best Thresholds per label]\")\n",
    "for k, v in thresholds.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 6) 평가\n",
    "# -------------------------------\n",
    "print(\"\\n=== Threshold-based ===\")\n",
    "print(f\"Micro-F1: {f1_score(y_val, y_val_pred_opt, average='micro'):.4f}\")\n",
    "print(f\"Macro-F1: {f1_score(y_val, y_val_pred_opt, average='macro'):.4f}\")\n",
    "print(f\"Sample-F1: {f1_score(y_val, y_val_pred_opt, average='samples'):.4f}\")\n",
    "print(\"\\n[classification_report @thr]\")\n",
    "print(classification_report(y_val, y_val_pred_opt, target_names=mlb.classes_, zero_division=0))\n",
    "\n",
    "# Top-K 기반 예측도 비교\n",
    "topk_preds = np.argsort(-y_val_proba, axis=1)[:, :TOP_K]\n",
    "topk_bin = np.zeros_like(y_val)\n",
    "for i, preds in enumerate(topk_preds):\n",
    "    topk_bin[i, preds] = 1\n",
    "\n",
    "print(\"\\n=== Top-K-based ===\")\n",
    "print(f\"Micro-F1: {f1_score(y_val, topk_bin, average='micro'):.4f}\")\n",
    "print(f\"Macro-F1: {f1_score(y_val, topk_bin, average='macro'):.4f}\")\n",
    "print(f\"Sample-F1: {f1_score(y_val, topk_bin, average='samples'):.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7) 예측 함수\n",
    "# -------------------------------\n",
    "def predict_multilingual(text: str, topk=3, thresholds=None):\n",
    "    text = preprocess_text(text, brand_list)  # ✨ 입력도 전처리 적용\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_LEN).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        proba = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "\n",
    "    if thresholds is not None:\n",
    "        pick = [i for i, p in enumerate(proba) if p >= thresholds.get(mlb.classes_[i], 0.5)]\n",
    "        if not pick:\n",
    "            pick = np.argsort(-proba)[:topk]\n",
    "    else:\n",
    "        pick = np.argsort(-proba)[:topk]\n",
    "\n",
    "    return [mlb.classes_[i] for i in pick]\n",
    "\n",
    "# -------------------------------\n",
    "# 예시 실행\n",
    "# -------------------------------\n",
    "print(\"\\n[Example Prediction]\")\n",
    "print(predict_multilingual(\"바닷가에서 느껴지는 시원하고 약간 달콤한 향이 좋아요\", topk=3, thresholds=thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6f771f8-94b4-4cde-9ca4-5341ae4a23dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.4-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.3.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.25.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.16.1)\n",
      "Downloading xgboost-3.0.4-py3-none-manylinux_2_28_x86_64.whl (94.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a521f4b4-9194-4dff-b495-75d6fe62d8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] cuda\n",
      "[Encoding Train Texts]\n",
      "[Encoding Validation Texts]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [01:40:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [01:40:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [01:40:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [01:40:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [01:40:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [01:40:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [01:40:15] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [01:40:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [01:40:15] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [01:40:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [01:40:15] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [01:40:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [01:40:15] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [01:40:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# MiniLM 임베딩 + XGBoost 분류기 + Threshold 최적화\n",
    "# ============================================\n",
    "\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# -------------------------------\n",
    "# 설정\n",
    "# -------------------------------\n",
    "DATA_CSV = \"perfumes_huggingface.csv\"\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "TOP_K = 3\n",
    "RARE_MIN_COUNT = 7\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[Device] {device}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 유틸 함수\n",
    "# -------------------------------\n",
    "def split_labels(s: str):\n",
    "    s = str(s)\n",
    "    for sep in [\",\", \"|\", \"/\", \";\"]:\n",
    "        s = s.replace(sep, \" \")\n",
    "    return [t.strip() for t in s.split() if t.strip()]\n",
    "\n",
    "# ✨ 전처리 함수 (불용어 단어 단위 매칭)\n",
    "def preprocess_text(text: str, brand_list=None):\n",
    "    text = str(text).lower()\n",
    "\n",
    "    # 브랜드명 제거\n",
    "    if brand_list:\n",
    "        for brand in brand_list:\n",
    "            text = re.sub(rf\"\\b{re.escape(brand.lower())}\\b\", \" \", text)\n",
    "\n",
    "    # 불필요한 특수문자 제거\n",
    "    text = re.sub(r\"[^a-zA-Z가-힣0-9\\s]\", \" \", text)\n",
    "\n",
    "    # 불용어 리스트\n",
    "    stopwords = {\n",
    "        # 관사\n",
    "        \"the\", \"a\", \"an\",\n",
    "    \n",
    "        # 접속사\n",
    "        \"and\", \"or\", \"but\", \"if\", \"while\", \"though\",\n",
    "    \n",
    "        # 전치사\n",
    "        \"of\", \"to\", \"in\", \"for\", \"on\", \"at\", \"by\", \"from\", \"into\", \"onto\",\n",
    "        \"with\", \"about\", \"over\", \"under\", \"between\", \"through\", \"without\",\n",
    "    \n",
    "        # 대명사\n",
    "        \"i\", \"me\", \"my\", \"mine\", \"you\", \"your\", \"yours\",\n",
    "        \"he\", \"him\", \"his\", \"she\", \"her\", \"hers\",\n",
    "        \"it\", \"its\", \"we\", \"us\", \"our\", \"ours\",\n",
    "        \"they\", \"them\", \"their\", \"theirs\",\n",
    "    \n",
    "        # 기타 불필요 단어\n",
    "        \"is\", \"am\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
    "        \"do\", \"does\", \"did\", \"done\",\n",
    "        \"have\", \"has\", \"had\", \"having\",\n",
    "        \"will\", \"would\", \"shall\", \"should\", \"can\", \"could\", \"may\", \"might\", \"must\",\n",
    "    \n",
    "        # 한정사 / 지시어\n",
    "        \"this\", \"that\", \"these\", \"those\", \"such\", \"some\", \"any\", \"each\", \"every\",\n",
    "        \"other\", \"another\", \"all\", \"both\", \"few\", \"many\", \"more\", \"most\", \"much\", \"no\", \"nor\"\n",
    "    }\n",
    "\n",
    "    for sw in stopwords:\n",
    "        text = re.sub(rf\"\\b{re.escape(sw)}\\b\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# -------------------------------\n",
    "# 1) 데이터 로드 & 전처리\n",
    "# -------------------------------\n",
    "df = pd.read_csv(DATA_CSV, sep=\"|\", engine=\"python\", on_bad_lines=\"skip\")\n",
    "df = df[~df[\"description\"].isna()].copy()\n",
    "df[\"labels\"] = df[\"fragrances\"].apply(split_labels)\n",
    "\n",
    "# 브랜드명 리스트\n",
    "brand_list = df[\"brand\"].dropna().unique().tolist() if \"brand\" in df.columns else []\n",
    "\n",
    "# 전처리\n",
    "df[\"description\"] = df[\"description\"].apply(lambda x: preprocess_text(x, brand_list))\n",
    "\n",
    "# 희소 라벨 제거\n",
    "cnt = Counter([l for L in df[\"labels\"] for l in L])\n",
    "rare = {k for k, v in cnt.items() if v <= RARE_MIN_COUNT}\n",
    "df[\"labels\"] = df[\"labels\"].apply(lambda L: [l for l in L if l not in rare])\n",
    "df = df[df[\"labels\"].map(len) > 0].copy()\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(df[\"labels\"])\n",
    "\n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
    "    df[\"description\"].tolist(), Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 2) MiniLM 임베딩 추출\n",
    "# -------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "base_model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "base_model.eval()\n",
    "\n",
    "def encode_texts(texts, batch_size=32):\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(batch, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            model_out = base_model(**enc)\n",
    "            emb = model_out.last_hidden_state.mean(dim=1)  # [CLS] pooling 대신 평균 풀링\n",
    "        all_embeddings.append(emb.cpu().numpy())\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "print(\"[Encoding Train Texts]\")\n",
    "X_train_emb = encode_texts(X_train_text, batch_size=BATCH_SIZE)\n",
    "print(\"[Encoding Validation Texts]\")\n",
    "X_val_emb = encode_texts(X_val_text, batch_size=BATCH_SIZE)\n",
    "\n",
    "# -------------------------------\n",
    "# 3) XGBoost 분류기 학습 (OvR)\n",
    "# -------------------------------\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    use_label_encoder=False,\n",
    "    tree_method=\"gpu_hist\" if device==\"cuda\" else \"hist\"\n",
    ")\n",
    "\n",
    "# 멀티라벨은 각 라벨마다 One-vs-Rest 방식으로 학습\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "clf = OneVsRestClassifier(xgb, n_jobs=-1)\n",
    "clf.fit(X_train_emb, y_train)\n",
    "\n",
    "# -------------------------------\n",
    "# 4) 검증 예측 & Threshold 최적화\n",
    "# -------------------------------\n",
    "y_val_proba = clf.predict_proba(X_val_emb)  # shape: (n_samples, n_classes)\n",
    "y_val_proba = np.array(y_val_proba)\n",
    "\n",
    "thresholds = {}\n",
    "y_val_pred_opt = np.zeros_like(y_val)\n",
    "\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    best_thr, best_f1 = 0.5, -1\n",
    "    for thr in np.linspace(0.2, 0.5, 16):\n",
    "        pred = (y_val_proba[:, i] >= thr).astype(int)\n",
    "        f1 = f1_score(y_val[:, i], pred, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_thr, best_f1 = thr, f1\n",
    "    thresholds[label] = best_thr\n",
    "    y_val_pred_opt[:, i] = (y_val_proba[:, i] >= best_thr).astype(int)\n",
    "\n",
    "print(\"\\n[Best Thresholds per label]\")\n",
    "for k, v in thresholds.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5) 평가\n",
    "# -------------------------------\n",
    "print(\"\\n=== Threshold-based ===\")\n",
    "print(f\"Micro-F1: {f1_score(y_val, y_val_pred_opt, average='micro'):.4f}\")\n",
    "print(f\"Macro-F1: {f1_score(y_val, y_val_pred_opt, average='macro'):.4f}\")\n",
    "print(f\"Sample-F1: {f1_score(y_val, y_val_pred_opt, average='samples'):.4f}\")\n",
    "print(\"\\n[classification_report @thr]\")\n",
    "print(classification_report(y_val, y_val_pred_opt, target_names=mlb.classes_, zero_division=0))\n",
    "\n",
    "# Top-K 기반 예측도 비교\n",
    "topk_preds = np.argsort(-y_val_proba, axis=1)[:, :TOP_K]\n",
    "topk_bin = np.zeros_like(y_val)\n",
    "for i, preds in enumerate(topk_preds):\n",
    "    topk_bin[i, preds] = 1\n",
    "\n",
    "print(\"\\n=== Top-K-based ===\")\n",
    "print(f\"Micro-F1: {f1_score(y_val, topk_bin, average='micro'):.4f}\")\n",
    "print(f\"Macro-F1: {f1_score(y_val, topk_bin, average='macro'):.4f}\")\n",
    "print(f\"Sample-F1: {f1_score(y_val, topk_bin, average='samples'):.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 6) 예측 함수\n",
    "# -------------------------------\n",
    "def predict_multilingual(text: str, topk=3, thresholds=None):\n",
    "    text = preprocess_text(text, brand_list)\n",
    "    emb = encode_texts([text], batch_size=1)\n",
    "    proba = clf.predict_proba(emb)[0]\n",
    "\n",
    "    if thresholds is not None:\n",
    "        pick = [i for i, p in enumerate(proba) if p >= thresholds.get(mlb.classes_[i], 0.5)]\n",
    "        if not pick:\n",
    "            pick = np.argsort(-proba)[:topk]\n",
    "    else:\n",
    "        pick = np.argsort(-proba)[:topk]\n",
    "\n",
    "    return [mlb.classes_[i] for i in pick]\n",
    "\n",
    "# -------------------------------\n",
    "# 예시 실행\n",
    "# -------------------------------\n",
    "print(\"\\n[Example Prediction]\")\n",
    "print(predict_multilingual(\"바닷가에서 느껴지는 시원하고 약간 달콤한 향이 좋아요\", topk=3, thresholds=thresholds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
