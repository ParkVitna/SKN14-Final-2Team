{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5365fb77-8e5e-4560-b8ee-4a44b7158f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers accelerate scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79198912-f160-404d-9236-00966939b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574df599-e323-48ed-bb79-13ac38914f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torch --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d3d76-dac8-4340-afce-c25ae528d3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 26319행 중, 빈 라벨 264행 제외 → 학습 행 26055\n",
      "라벨 개수: 17\n",
      "train: (23449, 6) valid: (2606, 6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15781a193ded4c82bb00982a8aff1bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7fcf48b29454e0eb58e14d49e721bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_3725/1001461248.py:148: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedBCETrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedBCETrainer(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:280: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6958' max='8796' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6958/8796 28:26 < 07:30, 4.08 it/s, Epoch 4.75/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.896800</td>\n",
       "      <td>1.133088</td>\n",
       "      <td>0.414247</td>\n",
       "      <td>0.228336</td>\n",
       "      <td>0.284178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.072400</td>\n",
       "      <td>1.127159</td>\n",
       "      <td>0.441635</td>\n",
       "      <td>0.267799</td>\n",
       "      <td>0.313997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.907600</td>\n",
       "      <td>1.256595</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.274405</td>\n",
       "      <td>0.360058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.875500</td>\n",
       "      <td>1.017803</td>\n",
       "      <td>0.461834</td>\n",
       "      <td>0.268472</td>\n",
       "      <td>0.348817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:280: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:280: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:280: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:280: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) 환경/경로 설정\n",
    "\n",
    "import os, json, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "DATA_PATH = \"hf_perfumes_mapped_ko.csv\"   # <- 필요시 경로 수정\n",
    "TEXT_COL  = \"description\"                  # 영어 문장\n",
    "LABEL_COL = \"main_accord_ko\"               # 한글 라벨(공백 구분)\n",
    "OUTPUT_DIR = \"./hf_xlmr_ckpt\"              # 모델 저장 폴더\n",
    "MODEL_NAME = \"xlm-roberta-base\"            # 다국어 베이스\n",
    "\n",
    "\n",
    "# 2) 데이터 로드 + 결측/빈 라벨 처리\n",
    "\n",
    "# 로드\n",
    "df = pd.read_csv(DATA_PATH, encoding=\"utf-8-sig\")\n",
    "df[LABEL_COL] = df[LABEL_COL].fillna(\"\").astype(str)\n",
    "\n",
    "# 공백 구분 다중라벨 -> 리스트\n",
    "def split_labels(s: str):\n",
    "    return [t for t in s.split() if t.strip()]\n",
    "\n",
    "df[\"labels_list\"] = df[LABEL_COL].apply(split_labels)\n",
    "\n",
    "# 학습셋: 라벨이 있는 행만 사용\n",
    "n_total = len(df)\n",
    "df_trainable = df[df[\"labels_list\"].map(len) > 0].copy()\n",
    "n_dropped = n_total - len(df_trainable)\n",
    "print(f\"총 {n_total}행 중, 빈 라벨 {n_dropped}행 제외 → 학습 행 {len(df_trainable)}\")\n",
    "\n",
    "# 라벨 인코딩\n",
    "all_labels = sorted({lab for labs in df_trainable[\"labels_list\"] for lab in labs})\n",
    "mlb = MultiLabelBinarizer(classes=all_labels)\n",
    "Y = mlb.fit_transform(df_trainable[\"labels_list\"])\n",
    "\n",
    "# train/valid split\n",
    "idx_tr, idx_va = train_test_split(\n",
    "    np.arange(len(df_trainable)),\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=(Y.sum(axis=1) > 0)\n",
    ")\n",
    "\n",
    "df_tr = df_trainable.iloc[idx_tr].reset_index(drop=True)\n",
    "df_va = df_trainable.iloc[idx_va].reset_index(drop=True)\n",
    "Y_tr, Y_va = Y[idx_tr], Y[idx_va]\n",
    "print(\"라벨 개수:\", len(all_labels))\n",
    "print(\"train:\", df_tr.shape, \"valid:\", df_va.shape)\n",
    "\n",
    "\n",
    "\n",
    "# 3) HF datasets 변환 & 토크나이즈\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "USE_TEXT_COL = TEXT_COL\n",
    "\n",
    "train_ds = Dataset.from_dict({\n",
    "    USE_TEXT_COL: df_tr[USE_TEXT_COL].astype(str).tolist(),\n",
    "    \"labels\": Y_tr.astype(\"float32\").tolist()\n",
    "})\n",
    "valid_ds = Dataset.from_dict({\n",
    "    USE_TEXT_COL: df_va[USE_TEXT_COL].astype(str).tolist(),\n",
    "    \"labels\": Y_va.astype(\"float32\").tolist()\n",
    "})\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tok(batch[USE_TEXT_COL], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "valid_ds = valid_ds.map(tokenize, batched=True)\n",
    "\n",
    "for ds in (train_ds, valid_ds):\n",
    "    ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
    "\n",
    "\"tokenized\"\n",
    "\n",
    "\n",
    "\n",
    "# 4) 모델 정의 & 학습\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import f1_score, jaccard_score\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(all_labels),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    micro = f1_score(labels, preds, average=\"micro\", zero_division=0)\n",
    "    macro = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "    jacc = jaccard_score(labels, preds, average=\"samples\")\n",
    "    return {\"micro_f1\": micro, \"macro_f1\": macro, \"jaccard\": jacc}\n",
    "\n",
    "# 라벨 불균형 가중치 (각 라벨마다)\n",
    "pos = Y_tr.sum(axis=0)                         # (L,)\n",
    "neg = len(Y_tr) - pos\n",
    "pos_weight = (neg / np.clip(pos, 1, None)).astype(\"float32\")\n",
    "\n",
    "class WeightedBCETrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs[\"labels\"]\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"]\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        loss_fct = torch.nn.BCEWithLogitsLoss(\n",
    "            pos_weight=torch.tensor(pos_weight, device=logits.device)\n",
    "        )\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=6,          \n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"micro_f1\",\n",
    "    greater_is_better=True,\n",
    "    warmup_ratio=0.06,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    bf16=True if torch.cuda.is_available() else False,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "# gradient_checkpointing 사용 시 권장 (경고 방지)\n",
    "model.config.use_cache = False\n",
    "\n",
    "trainer = WeightedBCETrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    tokenizer=tok,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 5) 체크포인트에 라벨/메타 저장\n",
    "\n",
    "\n",
    "import os, json\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "with open(os.path.join(OUTPUT_DIR, \"labels_ko.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_labels, f, ensure_ascii=False, indent=2)\n",
    "with open(os.path.join(OUTPUT_DIR, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"text_col\": USE_TEXT_COL}, f, ensure_ascii=False, indent=2)\n",
    "\"saved labels/meta\"\n",
    "\n",
    "\n",
    "\n",
    "# 6) 라벨별 임계값 최적화\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np, os\n",
    "\n",
    "pred = trainer.predict(valid_ds)\n",
    "probs = 1 / (1 + np.exp(-pred.predictions))   # (N,L)\n",
    "labels = pred.label_ids                        # (N,L)\n",
    "\n",
    "ths = np.arange(0.30, 0.71, 0.05)\n",
    "best_th_per_label = np.zeros(probs.shape[1], dtype=float) + 0.5\n",
    "\n",
    "for j in range(probs.shape[1]):\n",
    "    best_f1, best_th = -1, 0.5\n",
    "    for th in ths:\n",
    "        y_pred = (probs[:, j] >= th).astype(int)\n",
    "        f1 = f1_score(labels[:, j], y_pred, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_th = f1, th\n",
    "    best_th_per_label[j] = best_th\n",
    "\n",
    "np.save(os.path.join(OUTPUT_DIR, \"label_thresholds.npy\"), best_th_per_label)\n",
    "best_th_per_label[:10], float(best_th_per_label.mean())\n",
    "\n",
    "\n",
    "\n",
    "#  7) 추론(한국어 문장도 바로)\n",
    "\n",
    "import torch, numpy as np, json, os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "def load_model_for_infer(ckpt_dir=OUTPUT_DIR):\n",
    "    tok_i = AutoTokenizer.from_pretrained(ckpt_dir)\n",
    "    mdl_i = AutoModelForSequenceClassification.from_pretrained(ckpt_dir)\n",
    "    labels_ko = json.load(open(os.path.join(ckpt_dir, \"labels_ko.json\"), \"r\", encoding=\"utf-8\"))\n",
    "    th_path = os.path.join(ckpt_dir, \"label_thresholds.npy\")\n",
    "    per_label_th = np.load(th_path) if os.path.exists(th_path) else None\n",
    "    meta = json.load(open(os.path.join(ckpt_dir, \"meta.json\"), \"r\", encoding=\"utf-8\"))\n",
    "    return tok_i, mdl_i, labels_ko, per_label_th, meta\n",
    "\n",
    "tok_i, mdl_i, labels_ko, per_label_th, meta = load_model_for_infer(OUTPUT_DIR)\n",
    "\n",
    "def predict_accords(texts, use_per_label_threshold=True, base_threshold=0.5, topk=10):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    inputs = tok_i(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    with torch.no_grad():\n",
    "        logits = mdl_i(**inputs).logits\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "    outs = []\n",
    "    for i, p in enumerate(probs):\n",
    "        if use_per_label_threshold and per_label_th is not None:\n",
    "            mask = (p >= per_label_th)\n",
    "        else:\n",
    "            mask = (p >= base_threshold)\n",
    "        picked = [(labels_ko[j], float(p[j])) for j in np.where(mask)[0]]\n",
    "        top = sorted([(labels_ko[j], float(p[j])) for j in range(len(labels_ko))], key=lambda x: -x[1])[:topk]\n",
    "        outs.append({\"text\": texts[i], \"picked\": picked, \"topk\": top})\n",
    "    return outs\n",
    "\n",
    "# 예시: 한국어 문장\n",
    "ex = \"달콤한 바닐라에 따뜻한 스파이스가 포근하게 감싸는 겨울 향\"\n",
    "res = predict_accords(ex, use_per_label_threshold=True)\n",
    "res[0]\n",
    "\n",
    "\n",
    "# 8) (선택) 검증셋 지표 + 라벨별 리포트\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 0.5 기준 간단 리포트\n",
    "pred = trainer.predict(valid_ds)\n",
    "probs = 1 / (1 + np.exp(-pred.predictions))\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "print(\"== threshold 0.5 ==\")\n",
    "print(classification_report(pred.label_ids, preds, target_names=all_labels, zero_division=0))\n",
    "\n",
    "# per-label threshold 리포트 (있을 때)\n",
    "if per_label_th is not None:\n",
    "    preds2 = (probs >= per_label_th).astype(int)\n",
    "    print(\"== per-label thresholds ==\")\n",
    "    print(classification_report(pred.label_ids, preds2, target_names=all_labels, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98734c49-6a5b-4069-a727-bf6a8824ac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip 캐시 정리\n",
    "!pip cache purge\n",
    "\n",
    "# huggingface 모델 캐시 정리\n",
    "!rm -rf ~/.cache/huggingface\n",
    "\n",
    "# torch wheel 캐시 정리\n",
    "!rm -rf ~/.cache/torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9553adb-10df-46d4-9188-ebecfb74018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.2.2+cu118 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6905626-8a18-4e37-a6ec-b72eafecdc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb71686-5682-40e4-afc3-1312b2b9b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torchaudio==2.2.2+cu118 torchvision==0.17.2+cu118 --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a0cd168-5767-4788-b215-4d41af866301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2025.7.34)\n",
      "Collecting click (from sacremoses)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.5.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.67.1)\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf, click, sacremoses\n",
      "Successfully installed click-8.2.1 protobuf-6.32.0 sacremoses-0.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sacremoses protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba5ef2cb-5715-4d8c-be4e-4e6053bb8610",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m OUTPUT_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_xlmr_ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# 네가 학습시 저장한 경로\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m tok \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m mdl \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(OUTPUT_DIR)\n\u001b[1;32m      9\u001b[0m labels_ko \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels_ko.json\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:1138\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1140\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1141\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1142\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min order to use this tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1143\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2069\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2066\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2067\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2069\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2072\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2073\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2078\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2315\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2313\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2315\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2316\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[1;32m   2317\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2318\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2319\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2320\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/xlm/tokenization_xlm.py:246\u001b[0m, in \u001b[0;36mXLMTokenizer.__init__\u001b[0;34m(self, vocab_file, merges_file, unk_token, bos_token, sep_token, pad_token, cls_token, mask_token, additional_special_tokens, lang2id, id2lang, do_lowercase_and_remove_accent, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mja_word_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzh_word_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m vocab_handle:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(vocab_handle)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import json, numpy as np, os\n",
    "\n",
    "OUTPUT_DIR = \"hf_xlmr_ckpt\"  # 네가 학습시 저장한 경로\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "mdl = AutoModelForSequenceClassification.from_pretrained(OUTPUT_DIR)\n",
    "\n",
    "labels_ko = json.load(open(os.path.join(OUTPUT_DIR, \"labels_ko.json\"), \"r\", encoding=\"utf-8\"))\n",
    "per_label_th = np.load(os.path.join(OUTPUT_DIR, \"label_thresholds.npy\"))\n",
    "\n",
    "print(\"모델과 토크나이저 로드 성공!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e010b616-9a08-4d78-8225-dd1e865366cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d248be4-44e4-44e2-b5ca-dd2e536413c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: Optional[float] = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict[str, Any], str, NoneType] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: str = 'passive', log_level_replica: str = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: Optional[bool] = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, use_ipex: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: Optional[bool] = True, label_names: Optional[list[str]] = None, load_best_model_at_end: Optional[bool] = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = '', fsdp_min_num_params: int = 0, fsdp_config: Union[dict[str, Any], str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[dict, str, NoneType] = None, deepspeed: Union[dict, str, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: Optional[str] = 'length', report_to: Union[NoneType, str, list[str]] = None, ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, hub_revision: Optional[str] = None, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict[str, Any], str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list[str] = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: int = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: Optional[bool] = False, include_num_input_tokens_seen: Optional[bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: Optional[bool] = False, liger_kernel_config: Optional[dict[str, bool]] = None, eval_use_gather_object: Optional[bool] = False, average_tokens_across_devices: Optional[bool] = True) -> None\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86423348-ae0d-40ed-9be1-11ef9b916a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf hf_xlmr_ckpt/checkpoint-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a3c195-58e3-4fc8-8711-0f75957575a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
