{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a03d90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [13:59:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [13:59:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [13:59:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [13:59:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loaded model from ./models.pkl]\n",
      "Labels: ['Amber', 'Aromatic', 'Blossom', 'Bouquet', 'Citrus', 'Classical', 'Crisp', 'Dry', 'Floral', 'Flower', 'Fougère', 'Fresh', 'Fresher', 'Fruity', 'Gourmand', 'Green', 'Iris', 'Jasmine', 'Lily', 'Mossy', 'Musk', 'Orange', 'Rich', 'Richer', 'Rose', 'Soft', 'Spicy', 'Tuberose', 'Valley', 'Violet', 'Water', 'White', 'Woods', 'Woody']\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 저장된 VotingClassifier (.pkl) 불러오기 + 예측\n",
    "# ============================================\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# -------------------------------\n",
    "# 설정\n",
    "# -------------------------------\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "MAX_LEN = 256\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[Device] {device}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 1) 저장된 pkl 불러오기\n",
    "# -------------------------------\n",
    "SAVE_PKL = \"./models.pkl\"\n",
    "data = joblib.load(SAVE_PKL)\n",
    "\n",
    "clf = data[\"classifier\"]\n",
    "mlb = data[\"mlb\"]\n",
    "thresholds = data[\"thresholds\"]\n",
    "\n",
    "print(f\"[Loaded model from {SAVE_PKL}]\")\n",
    "print(f\"Labels: {list(mlb.classes_)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2) MiniLM 로드 (임베딩 추출용)\n",
    "# -------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "base_model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "base_model.eval()\n",
    "\n",
    "def encode_texts(texts, batch_size=32):\n",
    "    \"\"\"텍스트를 MiniLM 임베딩으로 변환\"\"\"\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(batch, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            model_out = base_model(**enc)\n",
    "            emb = model_out.last_hidden_state.mean(dim=1)\n",
    "        all_embeddings.append(emb.cpu().numpy())\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# -------------------------------\n",
    "# 3) 예측 함수\n",
    "# -------------------------------\n",
    "def predict_multilingual(text: str, topk=3, thresholds=None):\n",
    "    emb = encode_texts([text], batch_size=1)\n",
    "    proba = clf.predict_proba(emb)[0]\n",
    "\n",
    "    if thresholds is not None:\n",
    "        pick = [i for i, p in enumerate(proba) if p >= thresholds.get(mlb.classes_[i], 0.5)]\n",
    "        if not pick:  # 어떤 것도 threshold 못 넘으면 topk 선택\n",
    "            pick = np.argsort(-proba)[:topk]\n",
    "    else:\n",
    "        pick = np.argsort(-proba)[:topk]\n",
    "\n",
    "    return [mlb.classes_[i] for i in pick]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "849a123a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Example Prediction]\n",
      "['Amber', 'Fresher']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# 4) 예측 실행\n",
    "# -------------------------------\n",
    "example_text = \"달달한 향 추천좀\"\n",
    "print(\"\\n[Example Prediction]\")\n",
    "print(predict_multilingual(example_text, topk=3, thresholds=thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b6994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_perfume_simple(\n",
    "    user_text: str,\n",
    "    topk_labels: int = 3,\n",
    "    top_n_perfumes: int = 5,\n",
    "    use_thresholds: bool = True,\n",
    "    model_pkl_path: str = \"./models.pkl\",\n",
    "    perfume_json_path: str = \"perfumes.json\",\n",
    "    model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    max_len: int = 256,\n",
    "    global_threshold: float = 0.4,\n",
    "    min_labels: int = 0,\n",
    "    # --- NEW: abstain gates ---\n",
    "    domain_gate: bool = True,\n",
    "    domain_tau: float = 0.17,      # 코사인 유사도 하한 (0.18~0.25 권장)\n",
    "    max_proba_min: float = 0.60,   # 최대 확률 하한 (0.55~0.65 권장)\n",
    "    per_label_floor: float = 0.40  # per-label threshold의 최저 바닥값\n",
    "):\n",
    "    import json, numpy as np, torch, joblib\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    from rank_bm25 import BM25Okapi\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # 1) Load ML bundle\n",
    "    data = joblib.load(model_pkl_path)\n",
    "    clf = data[\"classifier\"]\n",
    "    mlb = data[\"mlb\"]\n",
    "    per_label_thresholds = (data.get(\"thresholds\", {}) or {})  # dict: label -> float\n",
    "\n",
    "    # 2) Encoder\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    enc_model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    enc_model.eval()\n",
    "\n",
    "    # 3) Load perfumes\n",
    "    with open(perfume_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        perfumes = json.load(f)\n",
    "        if not isinstance(perfumes, list):\n",
    "            raise ValueError(\"perfumes.json must contain a list of perfume objects\")\n",
    "\n",
    "    # 4) BM25 index\n",
    "    def doc_of(p):\n",
    "        fr = p.get(\"fragrances\")\n",
    "        if isinstance(fr, list):\n",
    "            text = \" \".join(map(str, fr))\n",
    "        elif isinstance(fr, str):\n",
    "            text = fr\n",
    "        else:\n",
    "            text = \" \".join(\n",
    "                str(x) for x in [\n",
    "                    p.get(\"description\", \"\"),\n",
    "                    p.get(\"main_accords\", \"\"),\n",
    "                    p.get(\"name_perfume\") or p.get(\"name\", \"\"),\n",
    "                    p.get(\"brand\", \"\"),\n",
    "                ] if x\n",
    "            )\n",
    "        return (text or \"unknown\").lower()\n",
    "\n",
    "    tokenized_corpus = [doc_of(p).split() for p in perfumes]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    # 5) Encode text -> vector\n",
    "    batch = tok([user_text], padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = enc_model(**batch)\n",
    "        emb = out.last_hidden_state.mean(dim=1).cpu().numpy()  # (1, d)\n",
    "\n",
    "    # --- Gate A: Domain similarity (쿼리 vs 도메인 대표문장) ---\n",
    "    if domain_gate:\n",
    "        ref_text = (\n",
    "            \"perfume fragrance scent cologne eau de parfum eau de toilette \"\n",
    "            \"citrus woody floral musk amber vanilla powdery aquatic green spicy \"\n",
    "            \"fresh sweet leather tobacco rose jasmine sandalwood patchouli vetiver bergamot lavender\"\n",
    "        )\n",
    "        ref_batch = tok([ref_text], padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            ref_out = enc_model(**ref_batch)\n",
    "            ref_emb = ref_out.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        # cosine similarity\n",
    "        def _cos(a, b):\n",
    "            a = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-12)\n",
    "            b = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-12)\n",
    "            return float((a @ b.T)[0, 0])\n",
    "        cos_sim = _cos(emb, ref_emb)\n",
    "        if cos_sim < float(domain_tau):\n",
    "            return {\n",
    "                \"user_input\": user_text,\n",
    "                \"predicted_labels\": [],\n",
    "                \"recommendations\": [],\n",
    "                \"meta\": {\n",
    "                    \"rejected\": True,\n",
    "                    \"reason\": \"DOMAIN_SIM_LOW\",\n",
    "                    \"cos_sim\": cos_sim,\n",
    "                    \"domain_tau\": float(domain_tau)\n",
    "                },\n",
    "            }\n",
    "\n",
    "    # 6) Predict probs\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        proba = clf.predict_proba(emb)[0]\n",
    "    elif hasattr(clf, \"decision_function\"):\n",
    "        logits = np.asarray(clf.decision_function(emb)[0], dtype=float)\n",
    "        proba = 1.0 / (1.0 + np.exp(-logits))\n",
    "    else:\n",
    "        proba = np.asarray(clf.predict(emb)[0], dtype=float)\n",
    "\n",
    "    # --- Gate B: Max probability lower bound ---\n",
    "    if float(np.max(proba)) < float(max_proba_min):\n",
    "        return {\n",
    "            \"user_input\": user_text,\n",
    "            \"predicted_labels\": [],\n",
    "            \"recommendations\": [],\n",
    "            \"meta\": {\n",
    "                \"rejected\": True,\n",
    "                \"reason\": \"LOW_CONFIDENCE_MAXP\",\n",
    "                \"max_proba\": float(np.max(proba)),\n",
    "                \"max_proba_min\": float(max_proba_min)\n",
    "            },\n",
    "        }\n",
    "\n",
    "    classes = list(mlb.classes_)\n",
    "\n",
    "    # === Threshold 기반 라벨 선택 (per-label 바닥값 적용) ===\n",
    "    if use_thresholds and per_label_thresholds:\n",
    "        th_vec = np.array([max(float(per_label_thresholds.get(c, global_threshold)), float(per_label_floor))\n",
    "                           for c in classes], dtype=float)\n",
    "    else:\n",
    "        th_vec = np.full_like(proba, fill_value=float(global_threshold), dtype=float)\n",
    "\n",
    "    picked_idx = [i for i, p in enumerate(proba) if p >= th_vec[i]]\n",
    "\n",
    "    # 최소 라벨 보장\n",
    "    if len(picked_idx) < int(min_labels):\n",
    "        order = np.argsort(-proba)\n",
    "        need = max(int(min_labels), min(len(order), int(topk_labels) if topk_labels else len(order)))\n",
    "        picked_idx = order[:need].tolist()\n",
    "\n",
    "    # 상한 컷\n",
    "    if topk_labels and len(picked_idx) > int(topk_labels):\n",
    "        picked_idx = sorted(picked_idx, key=lambda i: proba[i], reverse=True)[:int(topk_labels)]\n",
    "\n",
    "    labels = [classes[i] for i in picked_idx]\n",
    "\n",
    "    # --- Gate C: No labels → reject (fallback 금지) ---\n",
    "    if len(labels) == 0:\n",
    "        return {\n",
    "            \"user_input\": user_text,\n",
    "            \"predicted_labels\": [],\n",
    "            \"recommendations\": [],\n",
    "            \"meta\": {\n",
    "                \"rejected\": True,\n",
    "                \"reason\": \"EMPTY_LABELS_LOW_CONFIDENCE\"\n",
    "            },\n",
    "        }\n",
    "\n",
    "    # 7) Retrieve with BM25 (labels만 사용)\n",
    "    query_tokens = \" \".join(labels).lower().split()\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    top_idx = np.argsort(scores)[-top_n_perfumes:][::-1]\n",
    "\n",
    "    def _safe(d, *keys, default=\"N/A\"):\n",
    "        for k in keys:\n",
    "            if k in d and d[k] not in (None, \"\"):\n",
    "                return d[k]\n",
    "        return default\n",
    "\n",
    "    recs = []\n",
    "    for rnk, idx in enumerate(top_idx, 1):\n",
    "        p = perfumes[int(idx)]\n",
    "        fr = p.get(\"fragrances\")\n",
    "        fr_text = \", \".join(map(str, fr)) if isinstance(fr, list) else (fr if isinstance(fr, str) else _safe(p, \"main_accords\"))\n",
    "        recs.append({\n",
    "            \"rank\": int(rnk),\n",
    "            \"index\": int(idx),\n",
    "            \"score\": float(scores[int(idx)]),\n",
    "            \"brand\": _safe(p, \"brand\"),\n",
    "            \"name\": _safe(p, \"name_perfume\", \"name\"),\n",
    "            \"fragrances\": fr_text,\n",
    "            \"perfume_data\": p,\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"user_input\": user_text,\n",
    "        \"predicted_labels\": labels,\n",
    "        \"recommendations\": recs,\n",
    "        \"meta\": {\n",
    "                \"model_name\": model_name,\n",
    "                \"device\": device,\n",
    "                \"max_len\": int(max_len),\n",
    "                \"db_size\": int(len(perfumes)),\n",
    "                \"threshold_mode\": \"per-label\" if (use_thresholds and per_label_thresholds) else \"global\",\n",
    "                \"global_threshold\": float(global_threshold),\n",
    "                \"min_labels\": int(min_labels),\n",
    "                \"max_labels\": int(topk_labels) if topk_labels else None,\n",
    "                \"rejected\": False\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44cdc89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [16:17:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [16:17:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [16:17:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [16:17:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [16:17:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [16:17:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [16:17:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [16:17:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'user_input': 'example_text',\n",
       " 'predicted_labels': ['Fresher'],\n",
       " 'recommendations': [{'rank': 1,\n",
       "   'index': 12014,\n",
       "   'score': 1.6456789611296945,\n",
       "   'brand': 'Dolce & Gabbana',\n",
       "   'name': 'Light Blue Sun Pour Homme 2019',\n",
       "   'fragrances': 'Water Fresher',\n",
       "   'perfume_data': {'brand': 'Dolce & Gabbana',\n",
       "    'name_perfume': 'Light Blue Sun Pour Homme 2019',\n",
       "    'family': 'AROMATIC FOUGERE',\n",
       "    'subfamily': 'WATERY',\n",
       "    'fragrances': 'Water Fresher',\n",
       "    'ingredients': ['Bergamot',\n",
       "     'Oakmoss',\n",
       "     'Rosemary',\n",
       "     'Ozonic Notes',\n",
       "     'Cedarwood',\n",
       "     'Ginger',\n",
       "     'Grapefruit',\n",
       "     'Musk',\n",
       "     'Vanilla',\n",
       "     'Vetiver'],\n",
       "    'origin': 'Italy',\n",
       "    'gender': 'Male',\n",
       "    'years': '2019',\n",
       "    'description': 'On the enchanting island of Capri, two hearts race with the intoxicating magic of summer love. Hand in hand, skin warmed by the dazzling Mediterranean sun rays, their golden auras shimmer in the sun as they snatch playful kisses at every turn.',\n",
       "    'image_name': 'eqkqvkflfasnmdfjpzdukhil96xevu85aofliueg348tvukoxum12z71gqzu-w500-q85.jpg'}},\n",
       "  {'rank': 2,\n",
       "   'index': 12024,\n",
       "   'score': 1.6456789611296945,\n",
       "   'brand': 'Gucci',\n",
       "   'name': 'Gucci Guilty Platinum Pour Homme',\n",
       "   'fragrances': 'Woods Fresher',\n",
       "   'perfume_data': {'brand': 'Gucci',\n",
       "    'name_perfume': 'Gucci Guilty Platinum Pour Homme',\n",
       "    'family': 'WOODY',\n",
       "    'subfamily': 'CITRUS',\n",
       "    'fragrances': 'Woods Fresher',\n",
       "    'ingredients': ['Cedarwood',\n",
       "     'Lemon',\n",
       "     'Neroli',\n",
       "     'Lavender',\n",
       "     'Orange Blossom',\n",
       "     'Patchouli',\n",
       "     'Pink Pepper'],\n",
       "    'origin': 'Italy',\n",
       "    'gender': 'Male',\n",
       "    'years': '2016',\n",
       "    'description': 'Inspired by the world of Alessandro Michele, enjoy this Platinum limited edition version of Gucci Pour Homme.',\n",
       "    'image_name': 'izq1oyi2thxr9yhzr6ugtrs6w0u9z9hn8c4i5lj8kepmx1t0rjev86fnm6i4-w500-q85.jpg'}},\n",
       "  {'rank': 3,\n",
       "   'index': 21,\n",
       "   'score': 1.6456789611296945,\n",
       "   'brand': 'Guerlain',\n",
       "   'name': 'Mandarine Basilic Harvest 2023',\n",
       "   'fragrances': 'Citrus Fresher',\n",
       "   'perfume_data': {'brand': 'Guerlain',\n",
       "    'name_perfume': 'Mandarine Basilic Harvest 2023',\n",
       "    'family': 'CITRUS',\n",
       "    'subfamily': 'GREEN',\n",
       "    'fragrances': 'Citrus Fresher',\n",
       "    'ingredients': ['Mandarin', 'Green Tea', 'Woody Notes'],\n",
       "    'origin': 'France',\n",
       "    'gender': 'Female',\n",
       "    'years': '2023',\n",
       "    'description': '“Mandarine Basilic Harvest is a sparkling creation around a Marzolo mandarin that reveals a lively and exquisite bitterness,\" Delphine Jelk, Guerlain Perfumer.',\n",
       "    'image_name': 'z449j45k80kp8mbsawkwx8izz9cpg5gfoa93ojz7720t7lyw49y1q1dwru54-w500-q85.jpg'}},\n",
       "  {'rank': 4,\n",
       "   'index': 26217,\n",
       "   'score': 1.6456789611296945,\n",
       "   'brand': 'Nobile 1942',\n",
       "   'name': 'Cedro Atlas I Superhero',\n",
       "   'fragrances': 'Floral Fresher',\n",
       "   'perfume_data': {'brand': 'Nobile 1942',\n",
       "    'name_perfume': 'Cedro Atlas I Superhero',\n",
       "    'family': 'FLORAL',\n",
       "    'subfamily': 'CITRUS',\n",
       "    'fragrances': 'Floral Fresher',\n",
       "    'ingredients': ['Cedarwood',\n",
       "     'Freesia',\n",
       "     'Peach',\n",
       "     'Floral Notes',\n",
       "     'Lemon',\n",
       "     'Cinnamon',\n",
       "     'Musk',\n",
       "     'Tonka Bean',\n",
       "     'Vanilla'],\n",
       "    'origin': 'Italy',\n",
       "    'gender': 'Male',\n",
       "    'years': '2009',\n",
       "    'description': 'Superhero is perfect for school and during sport.',\n",
       "    'image_name': '8p2msdi7efiim4wjznknxpzg5q1vob8aow2ln19uexumwcg43rk246u0kaux-w500-q85.jpg'}},\n",
       "  {'rank': 5,\n",
       "   'index': 8,\n",
       "   'score': 1.6456789611296945,\n",
       "   'brand': 'Laura Biagiotti',\n",
       "   'name': 'Blu Di Roma Uomo',\n",
       "   'fragrances': 'Woods Fresher',\n",
       "   'perfume_data': {'brand': 'Laura Biagiotti',\n",
       "    'name_perfume': 'Blu Di Roma Uomo',\n",
       "    'family': 'WOODY',\n",
       "    'subfamily': 'CITRUS',\n",
       "    'fragrances': 'Woods Fresher',\n",
       "    'ingredients': ['Lemon',\n",
       "     'Leather',\n",
       "     'Grapefruit',\n",
       "     'Artemisia',\n",
       "     'Cardamom',\n",
       "     'Musk'],\n",
       "    'origin': 'Italy',\n",
       "    'gender': 'Male',\n",
       "    'years': '2014',\n",
       "    'description': 'Blu di Roma, a romantic, intense, and fresh fragrance inspired by the splendor of the blue shades of Roman summer, the enchantment of an adventure in the city that more than anything invites you to the dream of an endless holiday.',\n",
       "    'image_name': 'p6ggrfhlf6qs1uq797kkojgw7kujbaiy092tmaal5fho08dwr3bjkqs9lp53-w500-q85.jpg'}}],\n",
       " 'meta': {'model_name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
       "  'device': 'cpu',\n",
       "  'max_len': 256,\n",
       "  'db_size': 26319,\n",
       "  'threshold_mode': 'per-label',\n",
       "  'global_threshold': 0.4,\n",
       "  'min_labels': 0,\n",
       "  'max_labels': 3,\n",
       "  'rejected': False}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# 4) 예측 실행\n",
    "# -------------------------------\n",
    "example_text = \"LELU 추천좀\"\n",
    "recommend_perfume_simple(\"example_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01cebb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running comprehensive test with 15 queries...\n",
      "============================================================\n",
      "\n",
      "[Test 1/15] Query: 'I want a fresh and citrusy perfume for summer'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Status: SUCCESS\n",
      "  Labels: 2 (['Citrus', 'Fresher'])\n",
      "  Recommendations: 5\n",
      "  Top recommendation: Molton Brown - Sunlit Clementine & Vetiver Eau De Parfum\n",
      "\n",
      "[Test 2/15] Query: 'Looking for something woody and masculine'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Status: SUCCESS\n",
      "  Labels: 3 (['Aromatic', 'Fougère', 'Fresher'])\n",
      "  Recommendations: 5\n",
      "  Top recommendation: Avon - Full Speed Max Turbo\n",
      "\n",
      "[Test 3/15] Query: 'Sweet vanilla scent for evening wear'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Status: SUCCESS\n",
      "  Labels: 3 (['Floral', 'Fresher', 'Gourmand'])\n",
      "  Recommendations: 5\n",
      "  Top recommendation: Ramón Monegal - Flower Power\n",
      "\n",
      "[Test 4/15] Query: 'Floral fragrance for romantic dates'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Status: SUCCESS\n",
      "  Labels: 2 (['Floral', 'Fresher'])\n",
      "  Recommendations: 5\n",
      "  Top recommendation: Al Haramain - Coupé\n",
      "\n",
      "[Test 5/15] Query: 'Clean and powdery scent for office'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Status: SUCCESS\n",
      "  Labels: 2 (['Fresher', 'Woods'])\n",
      "  Recommendations: 5\n",
      "  Top recommendation: Axe - Axe Wild\n",
      "\n",
      "[Test 6/15] Query: 'something light and airy'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:30] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:30] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:30] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:30] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Status: SUCCESS\n",
      "  Labels: 2 (['Amber', 'Fresher'])\n",
      "  Recommendations: 5\n",
      "  Top recommendation: Tabac - Tabac Man Fire Power\n",
      "\n",
      "[Test 7/15] Query: 'I need good smell'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Status: REJECTED\n",
      "  Labels: 0 ([])\n",
      "  Recommendations: 0\n",
      "\n",
      "[Test 8/15] Query: 'nice scent please'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Status: REJECTED\n",
      "  Labels: 0 ([])\n",
      "  Recommendations: 0\n",
      "\n",
      "[Test 9/15] Query: 'perfume for special occasion'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Status: REJECTED\n",
      "  Labels: 0 ([])\n",
      "  Recommendations: 0\n",
      "\n",
      "[Test 10/15] Query: 'fragrance recommendation'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Status: REJECTED\n",
      "  Labels: 0 ([])\n",
      "  Recommendations: 0\n",
      "\n",
      "[Test 11/15] Query: 'I want to cook pasta today'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Status: REJECTED\n",
      "  Labels: 0 ([])\n",
      "  Recommendations: 0\n",
      "\n",
      "[Test 12/15] Query: 'How to fix my car engine'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:30:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Status: REJECTED\n",
      "  Labels: 0 ([])\n",
      "  Recommendations: 0\n",
      "\n",
      "[Test 13/15] Query: 'Best programming language to learn'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:31:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:31:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:31:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:31:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Status: REJECTED\n",
      "  Labels: 0 ([])\n",
      "  Recommendations: 0\n",
      "\n",
      "[Test 14/15] Query: 'Weather forecast for tomorrow'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:31:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:31:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:31:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:31:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Status: REJECTED\n",
      "  Labels: 0 ([])\n",
      "  Recommendations: 0\n",
      "\n",
      "[Test 15/15] Query: 'Math homework help needed'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:31:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:31:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:31:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:31:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:31:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:31:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:31:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [17:31:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Status: REJECTED\n",
      "  Labels: 0 ([])\n",
      "  Recommendations: 0\n",
      "\n",
      "\n",
      "Test completed! Results saved to keyword_test.txt\n",
      "Success: 6, Rejected: 9, Fallback: 0, Errors: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import joblib\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from rank_bm25 import BM25Okapi\n",
    "import pinecone\n",
    "from typing import List, Dict, Any, Optional\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PineconeKeywordVectorDB:\n",
    "    \"\"\"파인콘을 사용한 키워드 벡터 검색 시스템\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        api_key: str,\n",
    "        environment: str = \"us-west1-gcp\",\n",
    "        index_name: str = \"perfume-keywords\",\n",
    "        dimension: int = 384,\n",
    "        metric: str = \"cosine\"\n",
    "    ):\n",
    "        self.api_key = api_key\n",
    "        self.environment = environment\n",
    "        self.index_name = index_name\n",
    "        self.dimension = dimension\n",
    "        self.metric = metric\n",
    "        self.index = None\n",
    "        \n",
    "        # 파인콘 초기화\n",
    "        pinecone.init(api_key=api_key, environment=environment)\n",
    "        \n",
    "    def create_or_connect_index(self):\n",
    "        \"\"\"인덱스 생성 또는 연결\"\"\"\n",
    "        try:\n",
    "            if self.index_name not in pinecone.list_indexes():\n",
    "                pinecone.create_index(\n",
    "                    name=self.index_name,\n",
    "                    dimension=self.dimension,\n",
    "                    metric=self.metric\n",
    "                )\n",
    "                logger.info(f\"Created new Pinecone index: {self.index_name}\")\n",
    "            \n",
    "            self.index = pinecone.Index(self.index_name)\n",
    "            logger.info(f\"Connected to Pinecone index: {self.index_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error connecting to Pinecone: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def build_keyword_vectors(self, perfumes: List[Dict], model_name: str, max_len: int = 256):\n",
    "        \"\"\"향수 데이터로부터 키워드 벡터 생성 및 업로드\"\"\"\n",
    "        if not self.index:\n",
    "            raise RuntimeError(\"Index not connected. Call create_or_connect_index() first.\")\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name).to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        vectors_to_upsert = []\n",
    "        \n",
    "        for i, perfume in enumerate(perfumes):\n",
    "            # 키워드 텍스트 생성\n",
    "            keywords = self._extract_keywords(perfume)\n",
    "            keyword_text = \" \".join(keywords)\n",
    "            \n",
    "            # 벡터 임베딩 생성\n",
    "            batch = tokenizer([keyword_text], padding=True, truncation=True, \n",
    "                            max_length=max_len, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = model(**batch)\n",
    "                vector = output.last_hidden_state.mean(dim=1).cpu().numpy()[0].tolist()\n",
    "            \n",
    "            # 메타데이터 준비\n",
    "            metadata = {\n",
    "                \"perfume_index\": i,\n",
    "                \"brand\": perfume.get(\"brand\", \"\"),\n",
    "                \"name\": perfume.get(\"name_perfume\") or perfume.get(\"name\", \"\"),\n",
    "                \"keywords\": keywords[:10],  # 상위 10개 키워드만 저장\n",
    "                \"main_accords\": perfume.get(\"main_accords\", \"\"),\n",
    "                \"description\": perfume.get(\"description\", \"\")[:500]  # 설명은 500자로 제한\n",
    "            }\n",
    "            \n",
    "            vectors_to_upsert.append({\n",
    "                \"id\": f\"perfume_{i}\",\n",
    "                \"values\": vector,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "            \n",
    "            # 배치 업로드 (100개씩)\n",
    "            if len(vectors_to_upsert) >= 100:\n",
    "                self.index.upsert(vectors_to_upsert)\n",
    "                logger.info(f\"Uploaded batch of {len(vectors_to_upsert)} vectors\")\n",
    "                vectors_to_upsert = []\n",
    "        \n",
    "        # 남은 벡터들 업로드\n",
    "        if vectors_to_upsert:\n",
    "            self.index.upsert(vectors_to_upsert)\n",
    "            logger.info(f\"Uploaded final batch of {len(vectors_to_upsert)} vectors\")\n",
    "    \n",
    "    def _extract_keywords(self, perfume: Dict) -> List[str]:\n",
    "        \"\"\"향수 데이터에서 키워드 추출\"\"\"\n",
    "        keywords = []\n",
    "        \n",
    "        # 향료 정보\n",
    "        fragrances = perfume.get(\"fragrances\", [])\n",
    "        if isinstance(fragrances, list):\n",
    "            keywords.extend([str(f).lower().strip() for f in fragrances])\n",
    "        elif isinstance(fragrances, str):\n",
    "            keywords.extend([f.strip().lower() for f in fragrances.split(\",\") if f.strip()])\n",
    "        \n",
    "        # 메인 어코드\n",
    "        main_accords = perfume.get(\"main_accords\", \"\")\n",
    "        if main_accords:\n",
    "            keywords.extend([a.strip().lower() for a in str(main_accords).split(\",\") if a.strip()])\n",
    "        \n",
    "        # 브랜드와 이름\n",
    "        brand = perfume.get(\"brand\", \"\")\n",
    "        name = perfume.get(\"name_perfume\") or perfume.get(\"name\", \"\")\n",
    "        if brand:\n",
    "            keywords.append(brand.lower().strip())\n",
    "        if name:\n",
    "            keywords.extend([w.lower().strip() for w in str(name).split() if len(w) > 2])\n",
    "        \n",
    "        # 설명에서 키워드 추출 (간단한 방식)\n",
    "        description = perfume.get(\"description\", \"\")\n",
    "        if description:\n",
    "            # 향수 관련 키워드들만 추출\n",
    "            perfume_terms = [\n",
    "                \"fresh\", \"sweet\", \"woody\", \"floral\", \"citrus\", \"musky\", \"spicy\",\n",
    "                \"vanilla\", \"amber\", \"powdery\", \"aquatic\", \"green\", \"leather\",\n",
    "                \"tobacco\", \"rose\", \"jasmine\", \"sandalwood\", \"patchouli\", \"vetiver\",\n",
    "                \"bergamot\", \"lavender\", \"fruity\", \"oriental\", \"gourmand\"\n",
    "            ]\n",
    "            desc_words = str(description).lower().split()\n",
    "            keywords.extend([w for w in desc_words if w in perfume_terms])\n",
    "        \n",
    "        # 중복 제거 및 빈 문자열 제거\n",
    "        keywords = list(set([k for k in keywords if k and len(k) > 1]))\n",
    "        \n",
    "        return keywords\n",
    "    \n",
    "    def search_similar_perfumes(\n",
    "        self, \n",
    "        query_vector: np.ndarray, \n",
    "        top_k: int = 5,\n",
    "        min_score: float = 0.3\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"벡터 유사도로 향수 검색\"\"\"\n",
    "        if not self.index:\n",
    "            raise RuntimeError(\"Index not connected\")\n",
    "        \n",
    "        # 쿼리 실행\n",
    "        results = self.index.query(\n",
    "            vector=query_vector.tolist(),\n",
    "            top_k=top_k,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        \n",
    "        recommendations = []\n",
    "        for i, match in enumerate(results.matches):\n",
    "            if match.score >= min_score:\n",
    "                metadata = match.metadata\n",
    "                recommendations.append({\n",
    "                    \"rank\": i + 1,\n",
    "                    \"index\": metadata[\"perfume_index\"],\n",
    "                    \"score\": float(match.score),\n",
    "                    \"brand\": metadata.get(\"brand\", \"N/A\"),\n",
    "                    \"name\": metadata.get(\"name\", \"N/A\"),\n",
    "                    \"fragrances\": \", \".join(metadata.get(\"keywords\", [])),\n",
    "                    \"similarity_score\": float(match.score),\n",
    "                    \"search_method\": \"pinecone_vector_similarity\"\n",
    "                })\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "\n",
    "def recommend_perfume_with_fallback(\n",
    "    user_text: str,\n",
    "    topk_labels: int = 3,\n",
    "    top_n_perfumes: int = 5,\n",
    "    use_thresholds: bool = True,\n",
    "    model_pkl_path: str = \"./models.pkl\",\n",
    "    perfume_json_path: str = \"perfumes.json\",\n",
    "    model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    max_len: int = 256,\n",
    "    global_threshold: float = 0.4,\n",
    "    min_labels: int = 0,\n",
    "    # 기존 abstain gates\n",
    "    domain_gate: bool = True,\n",
    "    domain_tau: float = 0.17,\n",
    "    max_proba_min: float = 0.60,\n",
    "    per_label_floor: float = 0.40,\n",
    "    # 파인콘 설정\n",
    "    pinecone_api_key: Optional[str] = None,\n",
    "    pinecone_environment: str = \"us-west1-gcp\",\n",
    "    use_pinecone_fallback: bool = True,\n",
    "    pinecone_min_score: float = 0.3\n",
    "):\n",
    "    \"\"\"향수 추천 함수 - 파인콘 키워드 벡터DB 대체 포함\"\"\"\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # 1) Load ML bundle\n",
    "    data = joblib.load(model_pkl_path)\n",
    "    clf = data[\"classifier\"]\n",
    "    mlb = data[\"mlb\"]\n",
    "    per_label_thresholds = (data.get(\"thresholds\", {}) or {})\n",
    "\n",
    "    # 2) Encoder\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    enc_model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    enc_model.eval()\n",
    "\n",
    "    # 3) Load perfumes\n",
    "    with open(perfume_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        perfumes = json.load(f)\n",
    "        if not isinstance(perfumes, list):\n",
    "            raise ValueError(\"perfumes.json must contain a list of perfume objects\")\n",
    "\n",
    "    # 4) BM25 index\n",
    "    def doc_of(p):\n",
    "        fr = p.get(\"fragrances\")\n",
    "        if isinstance(fr, list):\n",
    "            text = \" \".join(map(str, fr))\n",
    "        elif isinstance(fr, str):\n",
    "            text = fr\n",
    "        else:\n",
    "            text = \" \".join(\n",
    "                str(x) for x in [\n",
    "                    p.get(\"description\", \"\"),\n",
    "                    p.get(\"main_accords\", \"\"),\n",
    "                    p.get(\"name_perfume\") or p.get(\"name\", \"\"),\n",
    "                    p.get(\"brand\", \"\"),\n",
    "                ] if x\n",
    "            )\n",
    "        return (text or \"unknown\").lower()\n",
    "\n",
    "    tokenized_corpus = [doc_of(p).split() for p in perfumes]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    # 5) Encode text -> vector\n",
    "    batch = tok([user_text], padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = enc_model(**batch)\n",
    "        emb = out.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "    # Gate A: Domain similarity\n",
    "    if domain_gate:\n",
    "        ref_text = (\n",
    "            \"perfume fragrance scent cologne eau de parfum eau de toilette \"\n",
    "            \"citrus woody floral musk amber vanilla powdery aquatic green spicy \"\n",
    "            \"fresh sweet leather tobacco rose jasmine sandalwood patchouli vetiver bergamot lavender\"\n",
    "        )\n",
    "        ref_batch = tok([ref_text], padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            ref_out = enc_model(**ref_batch)\n",
    "            ref_emb = ref_out.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "        def _cos(a, b):\n",
    "            a = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-12)\n",
    "            b = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-12)\n",
    "            return float((a @ b.T)[0, 0])\n",
    "\n",
    "        cos_sim = _cos(emb, ref_emb)\n",
    "        if cos_sim < float(domain_tau):\n",
    "            return {\n",
    "                \"user_input\": user_text,\n",
    "                \"predicted_labels\": [],\n",
    "                \"recommendations\": [],\n",
    "                \"meta\": {\n",
    "                    \"rejected\": True,\n",
    "                    \"reason\": \"DOMAIN_SIM_LOW\",\n",
    "                    \"cos_sim\": cos_sim,\n",
    "                    \"domain_tau\": float(domain_tau)\n",
    "                },\n",
    "            }\n",
    "\n",
    "    # 6) Predict probs\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        proba = clf.predict_proba(emb)[0]\n",
    "    elif hasattr(clf, \"decision_function\"):\n",
    "        logits = np.asarray(clf.decision_function(emb)[0], dtype=float)\n",
    "        proba = 1.0 / (1.0 + np.exp(-logits))\n",
    "    else:\n",
    "        proba = np.asarray(clf.predict(emb)[0], dtype=float)\n",
    "\n",
    "    # Gate B: Max probability lower bound\n",
    "    if float(np.max(proba)) < float(max_proba_min):\n",
    "        return {\n",
    "            \"user_input\": user_text,\n",
    "            \"predicted_labels\": [],\n",
    "            \"recommendations\": [],\n",
    "            \"meta\": {\n",
    "                \"rejected\": True,\n",
    "                \"reason\": \"LOW_CONFIDENCE_MAXP\",\n",
    "                \"max_proba\": float(np.max(proba)),\n",
    "                \"max_proba_min\": float(max_proba_min)\n",
    "            },\n",
    "        }\n",
    "\n",
    "    classes = list(mlb.classes_)\n",
    "\n",
    "    # 임계값 기반 라벨 선택\n",
    "    if use_thresholds and per_label_thresholds:\n",
    "        th_vec = np.array([max(float(per_label_thresholds.get(c, global_threshold)), float(per_label_floor))\n",
    "                           for c in classes], dtype=float)\n",
    "    else:\n",
    "        th_vec = np.full_like(proba, fill_value=float(global_threshold), dtype=float)\n",
    "\n",
    "    picked_idx = [i for i, p in enumerate(proba) if p >= th_vec[i]]\n",
    "\n",
    "    # 최소 라벨 보장\n",
    "    if len(picked_idx) < int(min_labels):\n",
    "        order = np.argsort(-proba)\n",
    "        need = max(int(min_labels), min(len(order), int(topk_labels) if topk_labels else len(order)))\n",
    "        picked_idx = order[:need].tolist()\n",
    "\n",
    "    # 상한 컷\n",
    "    if topk_labels and len(picked_idx) > int(topk_labels):\n",
    "        picked_idx = sorted(picked_idx, key=lambda i: proba[i], reverse=True)[:int(topk_labels)]\n",
    "\n",
    "    labels = [classes[i] for i in picked_idx]\n",
    "\n",
    "    # === 여기가 핵심: 빈 라벨일 때 파인콘 대체 시스템 ===\n",
    "    if len(labels) == 0 and use_pinecone_fallback and pinecone_api_key:\n",
    "        logger.info(\"Empty labels detected. Falling back to Pinecone keyword vector search.\")\n",
    "        \n",
    "        try:\n",
    "            # 파인콘 벡터DB 초기화\n",
    "            pinecone_db = PineconeKeywordVectorDB(\n",
    "                api_key=pinecone_api_key,\n",
    "                environment=pinecone_environment\n",
    "            )\n",
    "            pinecone_db.create_or_connect_index()\n",
    "            \n",
    "            # 파인콘에서 유사도 검색\n",
    "            recommendations = pinecone_db.search_similar_perfumes(\n",
    "                query_vector=emb[0],\n",
    "                top_k=top_n_perfumes,\n",
    "                min_score=pinecone_min_score\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"user_input\": user_text,\n",
    "                \"predicted_labels\": [],\n",
    "                \"recommendations\": recommendations,\n",
    "                \"meta\": {\n",
    "                    \"model_name\": model_name,\n",
    "                    \"device\": device,\n",
    "                    \"rejected\": False,\n",
    "                    \"fallback_used\": \"pinecone_vector_similarity\",\n",
    "                    \"pinecone_min_score\": pinecone_min_score,\n",
    "                    \"original_reason\": \"EMPTY_LABELS_LOW_CONFIDENCE\"\n",
    "                },\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pinecone fallback failed: {e}\")\n",
    "            # 파인콘 실패 시 원래대로 빈 결과 반환\n",
    "            return {\n",
    "                \"user_input\": user_text,\n",
    "                \"predicted_labels\": [],\n",
    "                \"recommendations\": [],\n",
    "                \"meta\": {\n",
    "                    \"rejected\": True,\n",
    "                    \"reason\": \"EMPTY_LABELS_LOW_CONFIDENCE\",\n",
    "                    \"pinecone_error\": str(e)\n",
    "                },\n",
    "            }\n",
    "    \n",
    "    elif len(labels) == 0:\n",
    "        # 파인콘 사용하지 않거나 API 키가 없는 경우\n",
    "        return {\n",
    "            \"user_input\": user_text,\n",
    "            \"predicted_labels\": [],\n",
    "            \"recommendations\": [],\n",
    "            \"meta\": {\n",
    "                \"rejected\": True,\n",
    "                \"reason\": \"EMPTY_LABELS_LOW_CONFIDENCE\"\n",
    "            },\n",
    "        }\n",
    "\n",
    "    # 7) 기존 BM25 검색 (라벨이 있는 경우)\n",
    "    query_tokens = \" \".join(labels).lower().split()\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    top_idx = np.argsort(scores)[-top_n_perfumes:][::-1]\n",
    "\n",
    "    def _safe(d, *keys, default=\"N/A\"):\n",
    "        for k in keys:\n",
    "            if k in d and d[k] not in (None, \"\"):\n",
    "                return d[k]\n",
    "        return default\n",
    "\n",
    "    recs = []\n",
    "    for rnk, idx in enumerate(top_idx, 1):\n",
    "        p = perfumes[int(idx)]\n",
    "        fr = p.get(\"fragrances\")\n",
    "        fr_text = \", \".join(map(str, fr)) if isinstance(fr, list) else (fr if isinstance(fr, str) else _safe(p, \"main_accords\"))\n",
    "        recs.append({\n",
    "            \"rank\": int(rnk),\n",
    "            \"index\": int(idx),\n",
    "            \"score\": float(scores[int(idx)]),\n",
    "            \"brand\": _safe(p, \"brand\"),\n",
    "            \"name\": _safe(p, \"name_perfume\", \"name\"),\n",
    "            \"fragrances\": fr_text,\n",
    "            \"search_method\": \"bm25_with_ml_labels\"\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"user_input\": user_text,\n",
    "        \"predicted_labels\": labels,\n",
    "        \"recommendations\": recs,\n",
    "        \"meta\": {\n",
    "            \"model_name\": model_name,\n",
    "            \"device\": device,\n",
    "            \"max_len\": int(max_len),\n",
    "            \"db_size\": int(len(perfumes)),\n",
    "            \"threshold_mode\": \"per-label\" if (use_thresholds and per_label_thresholds) else \"global\",\n",
    "            \"global_threshold\": float(global_threshold),\n",
    "            \"min_labels\": int(min_labels),\n",
    "            \"max_labels\": int(topk_labels) if topk_labels else None,\n",
    "            \"rejected\": False\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "# === 테스트 코드 ===\n",
    "def generate_test_queries():\n",
    "    \"\"\"테스트 쿼리 생성\"\"\"\n",
    "    test_queries = [\n",
    "        # 향수 도메인 관련 (정상적으로 처리되어야 함)\n",
    "        \"I want a fresh and citrusy perfume for summer\",\n",
    "        \"Looking for something woody and masculine\",\n",
    "        \"Sweet vanilla scent for evening wear\",\n",
    "        \"Floral fragrance for romantic dates\",\n",
    "        \"Clean and powdery scent for office\",\n",
    "        \n",
    "        # 경계선 케이스 (낮은 확신도로 빈 라벨 가능성)\n",
    "        \"something light and airy\",\n",
    "        \"I need good smell\",\n",
    "        \"nice scent please\",\n",
    "        \"perfume for special occasion\",\n",
    "        \"fragrance recommendation\",\n",
    "        \n",
    "        # 도메인 밖 쿼리 (도메인 게이트에서 거부되어야 함)\n",
    "        \"I want to cook pasta today\",\n",
    "        \"How to fix my car engine\",\n",
    "        \"Best programming language to learn\",\n",
    "        \"Weather forecast for tomorrow\",\n",
    "        \"Math homework help needed\"\n",
    "    ]\n",
    "    return test_queries\n",
    "\n",
    "def run_comprehensive_test(\n",
    "    model_pkl_path: str = \"./models.pkl\",\n",
    "    perfume_json_path: str = \"perfumes.json\", \n",
    "    pinecone_api_key: str = None,\n",
    "    output_file: str = \"keyword_test.txt\"\n",
    "):\n",
    "    \"\"\"종합 테스트 실행 및 결과 저장\"\"\"\n",
    "    \n",
    "    test_queries = generate_test_queries()\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Running comprehensive test with {len(test_queries)} queries...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\n[Test {i}/{len(test_queries)}] Query: '{query}'\")\n",
    "        \n",
    "        try:\n",
    "            result = recommend_perfume_with_fallback(\n",
    "                user_text=query,\n",
    "                model_pkl_path=model_pkl_path,\n",
    "                perfume_json_path=perfume_json_path,\n",
    "                pinecone_api_key=pinecone_api_key,\n",
    "                use_pinecone_fallback=True if pinecone_api_key else False,\n",
    "                # 테스트를 위해 임계값을 높여서 빈 라벨 유도\n",
    "                max_proba_min=0.70,\n",
    "                global_threshold=0.50\n",
    "            )\n",
    "            \n",
    "            # 결과 분석\n",
    "            is_rejected = result[\"meta\"].get(\"rejected\", False)\n",
    "            fallback_used = result[\"meta\"].get(\"fallback_used\")\n",
    "            num_recommendations = len(result[\"recommendations\"])\n",
    "            num_labels = len(result[\"predicted_labels\"])\n",
    "            \n",
    "            status = \"REJECTED\" if is_rejected else \"SUCCESS\"\n",
    "            if fallback_used:\n",
    "                status += f\" (Fallback: {fallback_used})\"\n",
    "            \n",
    "            print(f\"  Status: {status}\")\n",
    "            print(f\"  Labels: {num_labels} ({result['predicted_labels']})\")\n",
    "            print(f\"  Recommendations: {num_recommendations}\")\n",
    "            \n",
    "            if result[\"recommendations\"]:\n",
    "                top_rec = result[\"recommendations\"][0]\n",
    "                print(f\"  Top recommendation: {top_rec['brand']} - {top_rec['name']}\")\n",
    "            \n",
    "            # 결과 저장\n",
    "            results.append({\n",
    "                \"query\": query,\n",
    "                \"result\": result,\n",
    "                \"test_summary\": {\n",
    "                    \"status\": status,\n",
    "                    \"num_labels\": num_labels,\n",
    "                    \"num_recommendations\": num_recommendations,\n",
    "                    \"is_rejected\": is_rejected,\n",
    "                    \"fallback_used\": fallback_used\n",
    "                }\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {e}\")\n",
    "            results.append({\n",
    "                \"query\": query,\n",
    "                \"error\": str(e),\n",
    "                \"test_summary\": {\n",
    "                    \"status\": \"ERROR\",\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    # 결과를 파일에 저장\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"PERFUME RECOMMENDATION SYSTEM - COMPREHENSIVE TEST RESULTS\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        f.write(f\"Total queries tested: {len(test_queries)}\\n\")\n",
    "        f.write(f\"Timestamp: {pd.Timestamp.now()}\\n\\n\")\n",
    "        \n",
    "        # 통계 요약\n",
    "        success_count = sum(1 for r in results if r.get(\"test_summary\", {}).get(\"status\", \"\").startswith(\"SUCCESS\"))\n",
    "        rejected_count = sum(1 for r in results if r.get(\"test_summary\", {}).get(\"is_rejected\", False))\n",
    "        fallback_count = sum(1 for r in results if r.get(\"test_summary\", {}).get(\"fallback_used\"))\n",
    "        error_count = sum(1 for r in results if \"error\" in r)\n",
    "        \n",
    "        f.write(\"SUMMARY STATISTICS:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        f.write(f\"Successful recommendations: {success_count}\\n\")\n",
    "        f.write(f\"Rejected queries: {rejected_count}\\n\")\n",
    "        f.write(f\"Fallback used: {fallback_count}\\n\")\n",
    "        f.write(f\"Errors: {error_count}\\n\\n\")\n",
    "        \n",
    "        # 상세 결과\n",
    "        f.write(\"DETAILED RESULTS:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\\n\")\n",
    "        \n",
    "        for i, result_data in enumerate(results, 1):\n",
    "            f.write(f\"[TEST {i}] Query: '{result_data['query']}'\\n\")\n",
    "            \n",
    "            if \"error\" in result_data:\n",
    "                f.write(f\"ERROR: {result_data['error']}\\n\\n\")\n",
    "                continue\n",
    "            \n",
    "            result = result_data[\"result\"]\n",
    "            summary = result_data[\"test_summary\"]\n",
    "            \n",
    "            f.write(f\"Status: {summary['status']}\\n\")\n",
    "            f.write(f\"Predicted Labels ({summary['num_labels']}): {result['predicted_labels']}\\n\")\n",
    "            f.write(f\"Recommendations ({summary['num_recommendations']}):\\n\")\n",
    "            \n",
    "            for rec in result[\"recommendations\"][:3]:  # 상위 3개만 표시\n",
    "                f.write(f\"  {rec['rank']}. {rec['brand']} - {rec['name']} (Score: {rec['score']:.3f})\\n\")\n",
    "            \n",
    "            f.write(f\"Meta: {json.dumps(result['meta'], indent=2, ensure_ascii=False)}\\n\")\n",
    "            f.write(\"\\n\" + \"=\"*60 + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"\\n\\nTest completed! Results saved to {output_file}\")\n",
    "    print(f\"Success: {success_count}, Rejected: {rejected_count}, Fallback: {fallback_count}, Errors: {error_count}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    # 파인콘 API 키 설정 (환경변수에서 가져오거나 직접 입력)\n",
    "    PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\", \"your-pinecone-api-key-here\")\n",
    "    \n",
    "    # 테스트 실행\n",
    "    try:\n",
    "        test_results = run_comprehensive_test(\n",
    "            model_pkl_path=\"./models.pkl\",\n",
    "            perfume_json_path=\"perfumes.json\",\n",
    "            pinecone_api_key=PINECONE_API_KEY if PINECONE_API_KEY != \"your-pinecone-api-key-here\" else None,\n",
    "            output_file=\"keyword_test.txt\"\n",
    "        )\n",
    "        \n",
    "        # 파인콘 인덱스 초기 구축 (한 번만 실행)\n",
    "        if PINECONE_API_KEY and PINECONE_API_KEY != \"your-pinecone-api-key-here\":\n",
    "            print(\"\\nBuilding Pinecone keyword vector index...\")\n",
    "            with open(\"perfumes.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "                perfumes = json.load(f)\n",
    "            \n",
    "            pinecone_db = PineconeKeywordVectorDB(api_key=PINECONE_API_KEY)\n",
    "            pinecone_db.create_or_connect_index()\n",
    "            pinecone_db.build_keyword_vectors(\n",
    "                perfumes=perfumes,\n",
    "                model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "            )\n",
    "            print(\"Pinecone index building completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Test execution failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
